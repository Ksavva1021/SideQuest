\chapter{\texorpdfstring{Search for extended Higgs sector signatures in $\PGt^+\PGt^-\PGt^+\PGt^-$ final states}{Search for extended Higgs sector signatures in tautautautau final states}}
\chaptermark{Extended Higgs sector search}  
\thispagestyle{plain}  % First page has default style
\pagestyle{chapterpages}
\label{Section:Chapter_4tau}
\minitoc

\section{Introduction}

Motivated by the theoretical considerations and the particularly intriguing measurement of the muon anomalous magnetic moment discussed in Section~\ref{Section:Chapter2_gminus2}, this chapter presents a detailed analysis targeting final states with four tau leptons $\PGt$ through the process $Z^*\rightarrow\phi A\rightarrow4\PGt$.

The analysis explores a region of parameter space that remains largely unconstrained by existing collider searches. This is primarily because the production mode proceeds via an off-shell $Z^*$ boson. This circumvents the dominant SM Higgs production mechanisms, which are already tightly constrained by current LHC measurements. As such, this search offers unique sensitivity to scenarios in extended Higgs sectors that could otherwise evade detection.

This chapter provides a comprehensive description of the analysis strategy employed in the CMS experiment to probe this signature. 
\section{Data and Simulation}
\subsection{Collision data}

This search is based on pp collision data collected by the CMS detector during the 2016-2018 Run 2 data-taking period. The collisions were recorded at a centre-of-mass energy $\sqrt{s} = 13\TeV$ and the full dataset corresponds to an integrated luminosity of approximately $138\unit{fb}^{-1}$.

\subsection{Backgrounds}
\label{Section:Chapter6_Backgrounds}
This section provides a brief overview of the SM processes that can contribute to the four-$\PGt$ signal region. The aim is to provide an understanding of how different backgrounds can enter the selection through the presence of genuine or misidentified $\PGt$ leptons. The specific treatment of each background category is described in detail in Section~\ref{Section:Chapter6_Background_Modelling}.

\textit{Diboson} production, specifically $\PZ\PZ \to 4\PGt$, constitutes the primary irreducible background in this analysis. These events contain four genuine $\PGt$ leptons in the final state, matching the signal topology by construction. Although the cross section is small compared to other SM processes, the kinematic features of $\PZ\PZ$ events make them difficult to distinguish from potential signal. In addition, $\PZ\PZ$ events with mixed-flavour decays can contribute irreducibly when the final state includes leptonic tau decays, such as $\PGt_e\PGt_e\PGt_h\PGt_h$ or $\PGt_\mu\PGt_\mu\PGt_h\PGt_h$. In such cases, the prompt electrons or muons from the $\PZ$ decay could mimic the kinematic signature of non-prompt leptons originating from $\PGt$ decays.

The \textit{\ac{DY}} process, $\PZ/\gamma^* \to \ell^+\ell^-$, is one of the dominant backgrounds in di-$\PGt$ final states, producing genuine $\PGt^+ \PGt^-$ pairs in approximately one-third of events. Although its impact is reduced in four-$\PGt$ final states, it remains relevant in cases where two genuine $\PGt$ leptons are produced and are accompanied by jets from ISR or FSR. These jets can be misidentified as $\PGt_h$ candidates, resulting in final states that pass the selection for four reconstructed $\PGt$ leptons. In addition, prompt electrons or muons from $\PZ/\gamma^* \to \Pe^+\Pe^-$ or $\mu^+\mu^-$ decays may be misidentified as $\PGt_h$ candidates. When combined with one or more jets that are also misidentified as $\PGt_h$, such events can satisfy the four-$\PGt$ selection criteria. Feynman diagrams illustrating DY production with and without ISR/FSR are shown in Figure~\ref{Figure:Chapter6_DY}.

\begin{figure}[h]
    \centering
    % First row
    \begin{subfigure}{0.45\textwidth}
        \centering
        \input{FeynmanDiagrams/DY.tex}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \input{FeynmanDiagrams/DY_ISR.tex}
        \caption{}
    \end{subfigure}

    \caption[Examples of Feynman diagrams for Drell-Yan without partons and one parton originating from initial state radiation.]{Examples of Feynman diagrams for Drell-Yan \textbf{(a)} without partons and \textbf{(b)} one parton originating from ISR.}
    \label{Figure:Chapter6_DY}
\end{figure}

\textit{Top quark pair production} ($\ttbar \to b\overline{b}W^+W^-$) can also lead to four-$\PGt$ final states when both $\PW$ bosons decay leptonically via $\PW \to \PGt \nu_\PGt$. This results in two genuine $\PGt$ leptons, while additional jets from the top decays can be misidentified as $\PGt_h$ candidates, thereby mimicking the four-$\PGt$ topology. Misidentification can also occur between tau decay modes; for example, a $\PGt_h$ may be reconstructed as $\PGt_e$ or $\PGt_\mu$, or vice versa. Furthermore, similar to the DY background, prompt electrons or muons originating from $\PW \to e/\mu \nu$ decays may be misidentified as $\PGt_h$ candidates. In such cases, when combined with genuine $\PGt$ leptons or additional misidentified jets, multiple misidentifications may result in an event satisfying the four-$\PGt$ selection criteria.

\textit{W+jets} events can contribute to the four-$\PGt$ signal region when the $\PW$ boson decays via $\PW \to \PGt \nu_\PGt$, producing a genuine $\PGt$ lepton, and the accompanying jets are misidentified as $\PGt_h$ candidates. Similar to $\ttbar$, additional contributions arise from misidentified prompt electrons or muons. Since these events typically contain only one genuine lepton, multiple jet misidentifications are again required to satisfy the four-$\PGt$ selection.

\textit{QCD-induced multijet} events can enter the signal region when multiple jets are simultaneously misidentified as $\PGt_h$ candidates. These events may also feature non-prompt electrons or muons from the decay of hadrons. Such leptons can be misidentified as genuine prompt leptons and reconstructed as $\PGt_e$ or $\PGt_\mu$ candidates, thereby mimicking tau decays.

\textit{Diboson} processes such as $\PW\PW$ and $\PW\PZ$ may contribute to the four-$\PGt$ signal region when one or more of the bosons decay leptonically via $\PW/\PZ \to \PGt$, yielding up to two or three genuine $\PGt$ leptons. To satisfy the selection, the remaining $\PGt$ candidates must arise from jets misidentified as $\PGt_h$, originating either from hadronic boson decays (e.g., $\PW \to q \bar{q}'$, $\PZ \to q \bar{q}$) or from ISR or FSR. Misidentification of prompt electrons or muons as $\PGt_h$ candidates, following the same mechanism described for DY, $\ttbar$, and W+jets backgrounds, can also contribute. \textit{Triboson} production (e.g., $\PW\PW\PZ$, $\PZ\PZ\PZ$) can similarly yield multiple genuine leptons, including $\PGt$ decays, and may enter the signal region through analogous misidentification pathways.

Other subdominant processes also contribute. \textit{Single-top} production may yield a genuine $\PGt$ from a $\PW \to \PGt \nu_\PGt$ decay. Still, due to the typically low jet multiplicity in these events, additional $\PGt_h$ candidates must arise from misidentified jets, often requiring ISR or FSR. Finally, \textit{electroweak production} of $\PZ$ or $\PW$ bosons in association with jets (\eg VBF-like $\PZ + jj$) can resemble the signal topology when real or misidentified $\PGt$ candidates are produced alongside forward jets. 

Although many of the backgrounds discussed above are partially reducible through the application of $\PGt$ identification algorithms, lepton isolation requirements, and $b$-jet vetoes, these techniques are not fully efficient, and some backgrounds remain irreducible. To summarise, a common feature across many of these background processes is the presence of prompt electrons or muons that can be reconstructed as $\PGt_e$, $\PGt_\mu$ or $\PGt_h$ candidates. This, in combination with jet misidentification, enables events with fewer than four genuine $\PGt$ leptons to satisfy the selection criteria. The specific treatment, estimation, and validation of each background category are described in detail in Section~\ref{Section:Chapter6_Background_Modelling}.

The simulated background processes used in this analysis are summarised in Table~\ref{Table:Chapter6_SimulatedBackgrounds}.

{
\centering
\setlength{\LTpost}{-2ex}  % tighten space after table
\small  % one size smaller than normal
\begin{longtable}{llc}
\caption[Summary of the simulated Standard Model backgrounds, including their generators and precision, used in the extended Higgs sector search.]
{Summary of the simulated SM backgrounds, including their generators and precision, used in the search. The following generators were used: 
\MADGRAPH~\cite{MadGraph} for leading-order matrix element calculations; 
\POWHEG~v1.0~\cite{Powheg_0} and v2.0~\cite{Powheg_1,Powheg_2,Powheg_3} for next-to-leading order processes including $\ttbar$ and single top; 
and \MGvATNLO~\cite{MadGraph} for diboson and triboson production. 
Parton showering and hadronisation were performed with \PYTHIA~\cite{PYTHIA}.}
\label{Table:Chapter6_SimulatedBackgrounds} \\
\hline
\textbf{Process} & \textbf{Generators} & \textbf{Cross section $\sigma$ [pb]} \\
\hline \hline
\endfirsthead

\hline
\textbf{Process} & \textbf{Generators} & \textbf{Cross section $\sigma$ [pb]} \\
\hline \hline
\endhead

\hline
\multicolumn{3}{r}{\textit{Continued on next page}} \\
\endfoot

\hline
\endlastfoot
\rowcolor{verylightblue}
\textbf{Drell-Yan, $\PZ/\gamma^* \to \ell^+ \ell^-$ (LO)\hyperlink{DY_W-MLM}{$^1$}} & & \\
+ jets, $10 < m_{\ell \ell} < 50\GeV$ & \MADGRAPH, \PYTHIA & 15810.0 (LO), 18610.0 (NLO) \\
+ jets, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 5379.0 (LO), 6077.2 (NNLO) \\
+1 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 997.3 (LO) \\
+2 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 347.0 (LO)\\
+3 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 126.4 (LO) \\
+4 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 71.7 (LO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{W+jets (LO)\hyperlink{DY_W-MLM}{$^1$}} & & \\
+ jets & \MADGRAPH, \PYTHIA & 52940.0 (LO), 61526.7 (NLO) \\
+1 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 9364.4 (LO) \\
+2 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 3168.6 (LO) \\
+3 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 1132.1 (LO) \\
+4 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 633.7 (LO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{\ttbar} & & \\
Fully hadronic & \POWHEG, \PYTHIA & 377.96 (NNLO)\\
Semi-leptonic & \POWHEG, \PYTHIA & 365.34 (NNLO)\\
Fully leptonic & \POWHEG, \PYTHIA & 88.29 (NNLO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Single top} & & \\
t-channel ($t$) & \POWHEG, \PYTHIA & 136.02 (NNLO) \\
t-channel ($\overline{t}$) & \POWHEG, \PYTHIA & 136.02 (NNLO) \\
$t + W^-$ & \POWHEG, \PYTHIA & 35.60 (NNLO) \\
$t + W^+$ & \POWHEG, \PYTHIA & 35.60 (NNLO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Diboson} & & \\
$\PW \PZ \rightarrow \ell \, \nu \, \nu \, \nu$  & \MCATNLO, \PYTHIA & 3.416 (NLO) \\
$\PW \PZ \rightarrow \ell \, \nu \, q \, q$        & \MCATNLO, \PYTHIA & 10.71 (NLO) \\
$\PW \PZ \rightarrow q \, q \, \ell \, \ell$            & \MCATNLO, \PYTHIA & 6.419 (NLO) \\
$\PW \PZ \rightarrow \ell \, \ell \,  \ell \, \nu $          & \MCATNLO, \PYTHIA & 5.213 (NLO) \\
$\PW \PW \rightarrow \ell \, \nu \, q \,q$        & \MCATNLO, \PYTHIA & 49.99 (NLO) \\
$\PW \PW \rightarrow \ell \, \nu \, \ell \, \nu$        & \POWHEG, \PYTHIA & 11.09 (NLO) \\
$\PZ \PZ \rightarrow \ell \, \ell \, \nu \, \nu$         & \POWHEG, \PYTHIA & 0.9740 (NLO) \\
\arrayrulecolor{lightgray}\hline
$\text{H}_{\text{VBF}} \rightarrow ZZ \rightarrow \ell \, \ell \, \ell \, \ell $ & \POWHEG, \PYTHIA & $1.040e^{-3}$ (NLO) \\
$\text{ggH} \rightarrow ZZ \rightarrow \ell \, \ell \, \ell \, \ell $ & \POWHEG, \PYTHIA & $1.333e^{-2}$ (NLO)\\
$qq \rightarrow ZZ \rightarrow \ell \, \ell \, \ell \, \ell $ & \POWHEG, \PYTHIA & 1.325 (NLO)\\
$gg \rightarrow ZZ \rightarrow e \, e \, \PGt \, \PGt $ & \PYTHIA & $3.194e^{-3}$ (NLO)\\
$gg \rightarrow ZZ \rightarrow \mu \, \mu \, \PGt \, \PGt $ & \PYTHIA & $3.194e^{-3}$ (NLO)\\
$gg \rightarrow ZZ \rightarrow \mu \, \mu \, \mu \, \mu $ & \PYTHIA & $1.585e^{-3}$ (NLO)\\
$gg \rightarrow ZZ \rightarrow \PGt \, \PGt \, \PGt \, \PGt $ & \PYTHIA & $1.585e^{-3}$ (NLO)\\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Triboson} & & \\
$\PW \PW \PZ $ & \MCATNLO, \PYTHIA & 0.1707 (NLO)\\
$\PW \PZ \PZ $ & \MCATNLO, \PYTHIA & 0.0571 (NLO)\\
$\PW \PW \PW $ & \MCATNLO, \PYTHIA & 0.2158 (NLO)\\
$\PZ \PZ \PZ $ & \MCATNLO, \PYTHIA & 0.0148 (NLO)\\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Pure Electroweak} & & \\
$\PW^+ \to \ell^+ \nu$ + 2 jets & \MADGRAPH, \PYTHIA & 25.62 (LO)\\
$\PW^- \to \ell^- \nu$ + 2 jets & \MADGRAPH, \PYTHIA & 20.25 (LO)\\
$\PZ \to \ell \ell$ + 2 jets & \MADGRAPH, \PYTHIA & 3.987 (LO)\\

\arrayrulecolor{black}\hline
\end{longtable}
}
\vspace{0.5em}
\noindent\begin{minipage}{\linewidth}
\footnotesize
\hypertarget{DY_W-MLM}{}$^{1}$ MLM jet matching scheme~\cite{MLM} is employed to avoid double counting between jets produced at the matrix element level and those generated during parton showering.\\
\hypertarget{DY_W-Stitch}{}$^{2}$ To improve statistical precision, exclusive samples with fixed jet multiplicities are generated using the MLM scheme. These are stitched with inclusive samples to reproduce the overall cross-section and kinematic distributions accurately.
\end{minipage}

% For \ac{NLO} simulations, however, the MLM scheme is incompatible with the negative event weights that naturally arise. In such cases, the FxFx jet merging scheme~\cite{FxFx} is used instead. 


% Although the background samples are generated at LO or NLO, the comparison of simulated events to data is performed using higher-order theoretical cross-sections. Specifically, $\PW$+jets, Z+jets, $\ttbar$, and single top quark events in the tW channel are normalised using \ac{NNLO} cross-sections~\cite{HighOrder_XS_1,HighOrder_XS_2,HighOrder_XS_3}, while single top and diboson events are normalised to cross-sections calculated at NLO~\cite{HighOrder_XS_3,HighOrder_XS_4,HighOrder_XS_5}.

\subsection{Modelling of Signal Processes}
\label{Section:Chapter6_SignalModelling}

Signal templates targeting the process $Z^* \to \phi A \to \PGt^+\PGt^-\PGt^+\PGt^-$ are modelled in the \MCATNLO (version 2.6.5) framework~\cite{MadGraph,FxFx} at NLO accuracy in QCD. The event generation is performed in the five-flavour scheme using the NNPDF3.1 PDFs~\cite{NNPDF}. Parton showering, hadronisation, and $\PGt$ lepton decays are simulated using \PYTHIA (version 8.230)~\cite{PYTHIA}, with the underlying event modelled according to the CP5 tune~\cite{CP5_Tune}. To reflect realistic running conditions, additional pp interactions are overlaid on simulated events according to the distribution observed in the Run 2 data-taking period. The detector response is simulated using the \GEANTfour-based CMS framework~\cite{GEANT4}, and events are reconstructed with the same software configuration as applied to collision data.

A two-dimensional mass grid is defined to ensure sensitivity across the allowed phase space. All masses are expressed in units of~\GeV. The grid consists of combinations of $m_A$ and $m_\phi$ as follows:

\begin{itemize}
    \item $m_A$: 40, 50, 60, 70, 80, 90, 100, 125, 140, 160, 200, 250, 300, 400, 600
    \item For each $m_A$, the following $m_\phi$ values are considered:
    \begin{itemize}
        \item $m_A = 40$--$90$: 60, 70, 80, 90, 100, 110, 125, 140, 160, 180, 200, 250, 300
        \item $m_A = 100$, $125$, $140$, $160$, $200$, $250$:  60-300 (as above), 400
        \item $m_A = 300$: 60–400 (as above), 600
        \item $m_A = 400$: 100, 110, 125, 140, 160, 180, 200, 250, 300, 400, 600
        \item $m_A = 600$: 400, 600, 800
    \end{itemize}
\end{itemize}

The signal is simulated under the \textit{alignment limit} of the 2HDM, consistent with current experimental constraints on the couplings of the observed Higgs boson. The specific scenario is dynamically selected at each mass point depending on the value of $m_\phi$:
\begin{itemize}
    \item For $m_\phi > 125~\GeV$, the heavier CP-even scalar is identified as $\phi \equiv H$
    \item For $m_\phi \leq 125~\GeV$, the lighter CP-even scalar is identified as $\phi \equiv h$
\end{itemize}

Model parameters that do not significantly influence the kinematic properties of the final state, such as the charged Higgs mass and the soft $\mathbb{Z}_2$-breaking term, are fixed to values that preserve perturbative unitarity. This also ensures theoretical consistency~\cite{TypeX_2HDM}. Variations of these parameters within their allowed ranges have negligible impact on the resulting signal kinematics. They are therefore set according to the following prescription:
\begin{equation_pad}
m_{H^\pm} = m_\phi, \quad\quad m_{12}^2 = m_\phi^2 \sin\beta \cos\beta.
\end{equation_pad}

Figure~\ref{Figure:Chapter6_GenVisDistributions} illustrates the generator-level distributions of the visible di-$\PGt$ mass for selected combinations of $m_\phi$ and $m_A$. This observable is defined as the invariant mass computed using only the visible decay products of the two $\PGt$ leptons (\ie excluding neutrinos). These examples illustrate the primary kinematic characteristics of the signal across the mass grid.

\begin{figure}[h]
        \centering
        % First row
        \begin{subfigure}[b]{0.7\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mvis_A.pdf}
            \caption{}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.7\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mvis_phi.pdf}
            \caption{}
        \end{subfigure}
    \caption[Generator-level visible mass distributions for different mass combinations of $\phi$ and A.]{Generator-level distributions of the visible di-$\PGt$ mass for selected combinations of $m_A$ and $m_\phi$. The visible mass is computed using only the decay products of the two $\PGt$ leptons, excluding neutrinos. Distributions are shown for scans over \textbf{(a)} $m_A$ and \textbf{(b)} $m_\phi$.}
    \label{Figure:Chapter6_GenVisDistributions}
\end{figure}
\clearpage

\subsubsection{Production Cross Sections and Branching Fractions}

The production cross section is computed at NLO and, within the alignment limit, is independent of $\tan\beta$. It varies across the mass plane, ranging from approximately $10~\unit{fb}$ at $(m_\phi, m_A) = (100, 60)~\GeV$ to $650~\unit{fb}$ at $(300, 160)~\GeV$. These values are summarised in Figure~\ref{Figure:Chapter6_ProductionXS}, which shows the cross section as a function of the two mass parameters in the alignment scenario.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.85\textwidth]{Figures/Chapter6/Production_XS.pdf}
    \caption[Production cross sections for $Z^* \to \phi A$ in the alignment limit.]{Production cross sections for $Z^ \to \phi A$ in the alignment limit at NLO, shown as a function of $m_\phi$ and $m_A$. The values are interpolated over a grid of simulated mass combinations.}
  \label{Figure:Chapter6_ProductionXS}
\end{figure}

Outside the alignment limit, the cross section acquires dependence on the scalar mixing angle. Specifically:
\begin{itemize}
    \item It scales with $\sin^2(\beta - \alpha)$ when $\phi \equiv H$ ($m_\phi > 125~\GeV$),
    \item It scales with $\cos^2(\beta - \alpha)$ when $\phi \equiv h$ ($m_\phi \leq 125~\GeV$).
\end{itemize}

The decay branching ratios of $\phi$ and $A$ into $\PGt^+\PGt^-$ are computed using \textsc{2HDECAY}~\cite{2HDECAY}. In the alignment limit, the $\PGt^+\PGt^-$ branching ratio of $A$ remains close to unity for $\tan\beta \gtrsim 2$, but decreases rapidly at lower $\tan\beta$, where hadronic decays such as $A \to b\bar{b}$ become dominant. A similar pattern holds for $\phi \to \PGt^+\PGt^-$, except when the mass difference $m_\phi - m_A$ exceeds $m_Z$. In such cases, the decay $\phi \to ZA$ becomes kinematically accessible and can significantly suppress the di-$\PGt$ branching fraction, even at large $\tan\beta$. These features are illustrated in Figure~\ref{Figure:Chapter6_BranchingFractions}, which shows representative branching fraction maps for selected mass configurations. 

\begin{figure}[h]
        \centering
        % First row
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/Phi_BR.pdf}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/A_BR.pdf}
            \caption{}
        \end{subfigure}
    \caption[Branching fractions of $\phi$ and $A$ to $\PGt^+\PGt^-$ for selected mass combinations.]{Branching fractions of \textbf{(a)} $\phi \to \PGt^+\PGt^-$ and \textbf{(b)} $A \to \PGt^+\PGt^-$ as a function of $m_\phi$ and $m_A$, computed using \textsc{2HDECAY} in the alignment limit.}
    \label{Figure:Chapter6_BranchingFractions}
\end{figure}

\section{Event selection strategy}
\label{sec:ObjectAndEventSelections}

Having established the motivation, signal modelling, and background composition, this section describes the strategy used to identify candidate $Z^* \to \phi A \to 4\PGt$ events. The final states considered consist of four tau leptons, each of which may decay either leptonically ($\PGt_e$ and $\PGt_{\mu}$) or hadronically ($\PGt_h$). Figure~\ref{Figure:Chapter6_4tau_decayModes_BF} displays a pie chart of the most frequent $4\PGt$ final states, constructed by enumerating all combinations of leptonic and hadronic tau decays and summing their relative contributions. The dominant modes are those containing two or more hadronic taus, which together comprise 87.1\% of all four-tau decays. These channels form the focus of the analysis. In contrast, Table~\ref{Table:Chapter6_4tau_decayModes_BF_Other} summarises rarer final states that are not considered in the analysis.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{Figures/Chapter6/pie_BF.pdf}
    \caption[Branching fractions of the dominant four-$\PGt$ decay modes.]
    {Branching fractions of the most frequent $4\PGt$ final states, constructed by enumerating all combinations of leptonic ($\PGt_e$, $\PGt_{\mu}$) and hadronic ($\PGt_h$) tau decays.}
  \label{Figure:Chapter6_4tau_decayModes_BF}
\end{figure}

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{12pt} % Increase column width
\arrayrulecolor{black} % Ensure outer border is black
\begin{tabular}{|c|c|}
\hline
Decay Mode                  & Branching Fraction {[}\%{]} \\ \hline \hline
$\PGt_e \PGt_e \PGt_{\mu} \PGt_h$             & 4.3 \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_{\mu} \PGt_{\mu} \PGt_h$                    & 4.2 \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{e} \PGt_h$                        & 1.5  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_{\mu} \PGt_{\mu} \PGt_{\mu} \PGt_h$                  & 1.4  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{\mu} \PGt_{\mu}$             & 0.6  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{e} \PGt_{\mu}$                     & 0.4  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_{\mu} \PGt_{\mu} \PGt_{\mu}$             & 0.4  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{e} \PGt_e$                  & 0.1  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_{\mu} \PGt_{\mu} \PGt_{\mu} \PGt_{\mu}$                  & 0.1  \\ 
\arrayrulecolor{black} \hline
\end{tabular}
\caption[Branching fractions of subdominant four-$\PGt$ decay modes]{Branching fractions of subdominant four-$\PGt$ final states, involving three or more leptonic $\PGt$ decays.}
\label{Table:Chapter6_4tau_decayModes_BF_Other}
\end{table}

In addition to focusing on the dominant final states with two or more hadronic taus, the analysis includes an orthogonal $\PGt_h\PGt_h\PGt_h$ channel. This channel targets events where one $\PGt_h$ in the fully hadronic $\PGt_h\PGt_h\PGt_h\PGt_h$ decay mode fails to be reconstructed, often due to limited trigger and identification efficiencies or high $p_\text{T}$ thresholds. In total, seven final states are retained for the analysis. Furthermore, a $\tau_\mu\tau_\mu\tau_\mu\tau_\mu$ control region is used to constrain the background prediction; this region is discussed in more detail in Section~\ref{Section:Chapter6_Background_Modelling}.

\subsection{Triggering}

Identifying events with four tau leptons poses a challenge, as there is no dedicated trigger that directly targets this specific final state. Instead, the selection relies on a combination of existing triggers designed to capture subsets of the decay products. The triggers considered include double-electron, double-muon, electron–muon, and single-lepton (electron or muon) triggers, as well as cross-triggers involving an electron or muon paired with a hadronic tau ($\tau_h$), specifically $e+\tau_h$ and $\mu+\tau_h$. Single-$\tau_h$ triggers, although available, are not used due to their high $p_T$ thresholds, which are too restrictive for the phase space targeted by this analysis. Cross-triggers and electron–muon triggers were also evaluated but found to offer negligible gains in signal acceptance. Therefore, they are excluded to simplify the calculation of trigger efficiencies. The set of triggers used, along with the relevant online $p_\text{T}$ thresholds for each data-taking period, is summarised in Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt} % Increase column width
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multirow{3}{*}{\text{Trigger}} 
& \multicolumn{6}{c|}{$p_\text{T}$ \text{Threshold (GeV)}} \\ \cline{2-7}
& \multicolumn{2}{c|}{\text{2016}} & \multicolumn{2}{c|}{\text{2017}} & \multicolumn{2}{c|}{\text{2018}} \\ \cline{2-7}
& \text{Obj$_1$} & \text{Obj$_2$} & \text{Obj$_1$} & \text{Obj$_2$} & \text{Obj$_1$} & \text{Obj$_2$} \\ \hline \hline
Single-Electron (e)                   & 26     & --     & 28     & --     & 33     & --     \\
\arrayrulecolor{lightgray} \hline
Single-Muon ($\mu$)                       & 23     & --     & 25     & --     & 25     & --     \\
\arrayrulecolor{lightgray} \hline
Double-Tau ($\PGt \PGt$)         & 40     & 40     & 40     & 40     & 40     & 40     \\
\arrayrulecolor{black} \hline
\end{tabular}
\caption{Table of minimum online $p_\text{T}$ thresholds (in GeV) for the triggers used in the analysis. Subcolumns refer to thresholds for the first and second trigger objects.}
\label{Table:Chapter6_TriggerThresholdsExpanded}
\end{table}

The specific trigger configuration used for each four-$\PGt$ final state is summarised in Table~\ref{Table:Chapter6_TriggersPerChannel}. These configurations are constructed as combinations of the single- and double-object triggers listed in Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}. For each event category, the trigger is considered to have fired if at least one of the listed object combinations satisfies the corresponding online threshold. The use of logical OR($\lor$) between object pairs reflects the inclusive nature of the selection.

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{|c|c|}
\hline
\text{Channel} & \text{Trigger Configuration} \\ \hline \hline

$\tau_h\tau_h\tau_h\tau_h$ &  
$\tau_1\tau_2 \mathbin{\lor} \tau_1\tau_3 \mathbin{\lor} \tau_1\tau_4 \mathbin{\lor} \tau_2\tau_3 \mathbin{\lor} \tau_2\tau_4 \mathbin{\lor} \tau_3\tau_4$ \\ 
\arrayrulecolor{lightgray} \hline

$\tau_h\tau_h\tau_h$ &
$\tau_1\tau_2 \mathbin{\lor} \tau_1\tau_3 \mathbin{\lor} \tau_2\tau_3$ \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_h\tau_h\tau_h$ &
$e_1 \mathbin{\lor} \tau_2\tau_3 \mathbin{\lor} \tau_2\tau_4 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_h\tau_h\tau_h$ &
$\mu_1 \mathbin{\lor} \tau_2\tau_3 \mathbin{\lor} \tau_2\tau_4 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_e\tau_h\tau_h$ &
$e_1 \mathbin{\lor} e_2 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_\mu\tau_h\tau_h$ &
$\mu_1 \mathbin{\lor} \mu_2 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_\mu\tau_h\tau_h$ &
$e_1 \mathbin{\lor} \mu_1 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu \tau_\mu \tau_\mu \tau_\mu$ &
$\mu_1 \mathbin{\lor} \mu_2 \mathbin{\lor} \mu_3 \mathbin{\lor} \mu_4$ \\
\arrayrulecolor{black} \hline

\end{tabular}
\caption{Trigger configuration used for each four-$\PGt$ final state.}
\label{Table:Chapter6_TriggersPerChannel}
\end{table}

\subsection{Offline object selections}
\label{sec:ObjectSelection}

The object selections defined in this section are applied to reconstructed electrons, muons, and hadronically decaying tau candidates ($\tauh$), as discussed in Chapter~\ref{Section:Chapter4}. These selections are designed to suppress backgrounds from misidentified and non-prompt objects, while retaining high signal efficiency. 

To reduce contamination from PU vertices, all leptons and $\tauh$ candidates are required to be consistent with the PV. This is enforced by applying cuts on the transverse and longitudinal impact parameters. Electrons and muons must satisfy $|d_{xy}| < 0.045\unit{cm}$ and $|d_z| < 0.2\unit{cm}$, while $\tauh$ candidates are required to have $|d_z| < 0.2~\unit{cm}$. 

Electrons and muons are additionally required to be isolated from surrounding hadronic activity in order to suppress backgrounds from non-prompt sources. The relative isolation variables used for this purpose are defined in Chapter~\ref{Section:Chapter4}, Equations~\ref{Equation:Chapter4_PFIso_Electron} and~\ref{Equation:Chapter4_PFIso_Muon}. A threshold of $I^{e/\mu}_\text{PF} < 0.15$ is applied to electrons and muons. For $\tauh$ candidates, no explicit isolation requirement is imposed, as isolation information is already incorporated into the DeepTau discriminators.

The identification criteria for electrons and muons follow the strategies described in Chapter~\ref{Section:Chapter4}, where electrons are identified using a multivariate (BDT-based) discriminator targeting a 90\% signal efficiency, and muons are selected using the cut-based medium working point.  For hadronically decaying tau leptons, a looser identification strategy is employed to ensure sufficient event yields. Specifically, $\tauh$ candidates are required to pass the \texttt{Loose} working point of the $D_{\text{jet}}$ discriminator. To further reduce contamination from electrons and muons misidentified as tau candidates, the \texttt{VVLoose} working point of the $D_e$ discriminator and the \texttt{VLoose} working point of the $D_\mu$ discriminator are employed. These working points are chosen as an optimal compromise between background rejection and signal retention, tailored to the statistical requirements of the analysis. For reference, the corresponding DeepTau score thresholds for these working points are listed in Table~\ref{Table:Chapter4_DeepTau_WPs}.

To ensure consistency between online and offline event selection, reconstructed objects in each event must match the trigger-level objects responsible for firing the trigger. Matching is performed using a cone radius of $\Delta R < 0.5$. The number of required matches varies depending on the trigger configuration used for each final state, as summarised in Table~\ref{Table:Chapter6_TriggersPerChannel}. For channels using double-object triggers (e.g.\ $\tauh\tauh\tauh\tauh$), it is sufficient for any two reconstructed $\tauh$ candidates to match the corresponding trigger objects. In contrast, for channels that rely on single-lepton triggers (e.g.\ $e\,\tauh\tauh\tauh$), only one reconstructed object may be required to match the trigger, depending on which trigger path fired. For matched objects, stricter offline $p_T$ thresholds are applied to ensure operation within the plateau of the trigger efficiency turn-on curve: a margin of $+1\GeV$ is added for electrons and muons, and $+5\GeV$ for $\tauh$ candidates. Objects not responsible for triggering the event (i.e.\ unmatched) are allowed to satisfy looser baseline $p_T$ thresholds. This conditional approach ensures optimal signal acceptance while maintaining accurate modelling of the trigger response.

The complete set of offline object selection criteria is summarised in Table~\ref{Table:Chapter6_ObjectSelectionSummary}.

{
\setlength{\arrayrulewidth}{1pt}

% Move the caption BEFORE the table
\begin{table}[h]
\centering
\caption[Summary of baseline object selection criteria]{
Summary of baseline selection criteria applied to reconstructed electrons, muons, and hadronically decaying tau candidates ($\tauh$). Trigger-matched $p_T$ thresholds are defined relative to those in Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}.
}
\label{Table:Chapter6_ObjectSelectionSummary}

\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\arrayrulecolor{black}

\begin{tabular}{cccc}
\hline
\textbf{Criteria} & \textbf{Electron} & \textbf{Muon} & \textbf{Hadronic Tau} \\
\hline

$p_\text{T}$  & > $10\GeV$ & > $10\GeV$ & > $20\GeV$\\ 
\arrayrulecolor{lightgray} \hline

$p_\text{T}^{\text{Trigger}}$ & \multicolumn{3}{c}{$> \text{Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}} + [1,\,1,\,5]$} \\
\arrayrulecolor{lightgray} \hline

$|\eta|$ & < $2.5$ & < $2.4$ & < $2.3$/$2.1$\hyperlink{DoubleTauTrigger-EtaCut}{$^1$} \\
\arrayrulecolor{lightgray} \hline

$|d_{xy}|$ & < $0.045\unit{cm}$ & < $0.045\unit{cm}$ & -- \\
\arrayrulecolor{lightgray} \hline

$|d_z|$ & < $0.2\unit{cm}$ & < $0.2\unit{cm}$ & < $0.2\unit{cm}$ \\
\arrayrulecolor{lightgray} \hline

Isolation & $I^e_\text{PF}$ < 0.15 & $I^\mu_\text{PF}$ < 0.15 & -- \\
\arrayrulecolor{lightgray} \hline

Identification
& \makecell{MVA w/o isolation\\(90\% WP)}
& Medium ID
& \makecell{
$D_{\text{jet}} \geq \texttt{Loose}$ \\
$D_{e} \geq \texttt{VVLoose}$ \\
$D_{\mu} \geq \texttt{VLoose}$
} \\
\arrayrulecolor{black} \hline
\end{tabular}
\vspace{0.5em}
\begin{minipage}{0.95\linewidth}
\raggedright
\footnotesize\hypertarget{DoubleTauTrigger-EtaCut}{}$^{1}$\,$|\eta| < 2.1$ is required for $\tauh$ candidates matched to trigger objects, reflecting the online trigger acceptance region.
\end{minipage}

\end{table}
}

\subsection{Event-level selections}
Beyond object-level requirements, additional criteria are imposed at the event level to enhance signal purity and enforce statistical orthogonality between final states. These include constraints on the number and charge of reconstructed objects, as well as vetoes on additional leptons and $b$-tagged jets. The complete set of event-level selections applied in each final state is summarised in Table~\ref{Table:Chapter6_Event_Channel_Selections}.

{
\setlength{\arrayrulewidth}{1pt}

% Move the caption BEFORE the table
\begin{table}[h]
\caption[Event-level selection requirements by channel.]{
Event-level selection criteria applied to each four-$\PGt$ final state. The table specifies the required number of electrons ($e$), muons ($\mu$), hadronic taus ($\tauh$), and $b$-tagged jets ($b_\text{jets}$), as well as the total charge requirement ($\sum q$) for the selected objects.}
\label{Table:Chapter6_Event_Channel_Selections}
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\arrayrulecolor{black}

\begin{tabular}{cccccc}
\hline
\textbf{Channel} & \textbf{$e$} & \textbf{$\mu$} & \textbf{$\tauh$} & \textbf{$b_\text{jets}$}\hyperlink{b-jet_selections}{$^1$} & \textbf{$\sum$\text{q}}\\
\hline

$\tau_h\tau_h\tau_h\tau_h$ &  0 & 0 & $\geq 4$ & $\geq 0$ & 0\\
\arrayrulecolor{lightgray} \hline

$\tau_h\tau_h\tau_h$ & 0 & 0 & 3 & $\geq 0$ & $\pm 1$\\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_h\tau_h\tau_h$ & 1 & 0 & $\geq3$ & $0$  & 0 \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_h\tau_h\tau_h$ & 0 & 1 & $\geq 3$ & $0$ & 0 \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_e\tau_h\tau_h$ & 2 & 0 & $\geq 2$ & $0$ & 0\\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_\mu\tau_h\tau_h$ & 0 & 2 & $\geq 2$ & $0$ & 0 \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_\mu\tau_h\tau_h$ & 1 & 1 & $\geq 2$ & $0$ & 0 \\
\arrayrulecolor{black} \hline
\end{tabular}
\vspace{0.5em}
\begin{minipage}{0.95\linewidth}
\raggedright
\footnotesize\hypertarget{b-jet_selections}{}$^{1}$\,Jets are required to pass the $b$-tagging criteria defined in Chapter~\ref{Section:Chapter4}, have $p_T > 30\GeV$, $|\eta| < 4.7$ and not overlap with any selected electron, muon or $\tauh$ candidates within $\Delta R < 0.5$.

\end{minipage}
\end{table}
}

These selections reflect the assumptions of the signal model and are tailored to optimise background rejection. In particular, $b$-jet vetoes are applied in channels with light leptonic tau decays to suppress $\ttbar$ contamination, while charge requirements ensure compatibility with the expected tau multiplicity and Higgs boson charge neutrality. The $\pm1$ charge condition in the $\tauh\tauh\tauh$ channel accounts for a potentially unreconstructed tau.

\section{Object and event corrections}

Simulated events are prone to imperfections, and discrepancies between simulation and data can arise from multiple sources. These include the limited accuracy of MC event generators, approximations in detector simulation, and differences in object reconstruction performance. In particular, simulated events may differ from real data in the rates at which objects pass selection criteria such as identification, isolation, and trigger requirements. Additionally, mismatches in detector response can lead to systematic shifts in reconstructed object energies.

To mitigate these effects, a series of corrections is applied to simulated events to improve agreement with observed data. These include object-level adjustments, such as energy scale and resolution corrections, as well as event-level weights derived from object-specific efficiency scale factors. For example, identification, isolation and trigger efficiencies are measured separately for each reconstructed object and are then combined multiplicatively to yield a total event weight correction. Additional event-level weights are applied to account for pileup reweighting and generator-level mismodelling.

Wherever possible, corrections derived centrally by the CMS collaboration are used. However, in cases where analysis-specific selections or trigger paths are employed, dedicated corrections are derived to ensure consistency with the data-taking conditions and selection strategy used in this search. The remainder of this section provides an overview of the key corrections applied to simulated events in this analysis.

\subsection{Pileup reweighting}

To ensure realistic event modelling, the distribution of pileup (PU) interactions per bunch crossing, referred to as the PU profile, must accurately reflect the conditions observed during data taking. In simulation, this PU profile is constructed by generating a distribution of the \textit{mean number of interactions per bunch crossing} for a given year. This distribution encodes the variation in instantaneous luminosity over the run period.

For each simulated event, a value of the mean number of interactions, denoted by $\mu^{\text{PU}}$, is randomly drawn from this reference PU profile. This $\mu^{\text{PU}}$ is then treated as the true mean of a Poisson distribution from which the actual number of PU interactions for the event is sampled. However, the PU profile used in simulation may only approximate the true distribution observed in data. To correct for this, PU reweighting is applied, where each simulated event is assigned a weight to match the PU distribution in simulation to that observed in data. The PU profiles for data and simulation used during Run 2 are shown in Fig.~\ref{Figure:Chapter6_PU_Profiles}, highlighting the need for reweighting to align the distributions.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{Figures/Chapter6/PU_Profile.pdf}
\caption{Comparison of the distributions of the mean number of proton-proton interactions per bunch crossing between data and simulation in CMS Run 2.}
\label{Figure:Chapter6_PU_Profiles}
\end{figure}

\subsection{Electron and Muon efficiencies}

While simulated leptons are reconstructed using algorithms that closely replicate those applied to real data, differences inevitably arise due to detector effects, reconstruction inefficiencies, and selection criteria. These differences are corrected through the application of efficiency \ac{SF}.

The efficiencies under consideration include tracking, identification, isolation, and trigger efficiencies. Tracking efficiencies are derived centrally by the CMS collaboration for both electrons and muons and are applied as provided. However, the remaining efficiencies are measured within the scope of this analysis using a dedicated implementation of the \textit{tag-and-probe} method~\cite{CMS_Muon_System_Performance,CMS_Muon_System_Performance_2}. This is necessary because of the object selection criteria in Table~\ref{Table:Chapter6_ObjectSelectionSummary}). In particular, the impact parameter requirements $d_{xy}$ and $d_z$, deviate from those used in the derivation of the official CMS corrections. 

The tag-and-probe technique is applied to leptons from $Z \rightarrow \ell^+\ell^-$ decays, providing a clean and unbiased sample for measuring the performance of individual selection stages. Events are selected by requiring a pair of opposite-charge, same-flavour leptons with an invariant mass $m_{\ell\ell}$ between $65\GeV$ and $115\GeV$, enhancing the purity of the $Z$ boson signal. One lepton in the pair, the \textit{tag}, must satisfy tight identification and isolation criteria as well as single-lepton trigger requirements. This ensures that the tag lepton does not bias the efficiency measurement of the second lepton, referred to as the \textit{probe}. The probe is then used to evaluate the efficiency of a specific selection requirement. To maximise statistical precision, both leptons are alternately treated as tag and probe. 

Efficiency measurements are performed in bins of the probe $p_{\mathrm{T}}$ and $\eta$, to account for variations in detector performance across different kinematic and detector regions. 

\begin{itemize}
    \item Identification efficiency:
    \begin{itemize}
        \item No isolation or trigger requirement is applied on the probe.
    \end{itemize}
    
    \item Isolation efficiency:
    \begin{itemize}
        \item The probe must satisfy the identification criteria.
        \item No trigger requirement is applied.
    \end{itemize}

    \item Trigger efficiency:
    \begin{itemize}
        \item The probe must satisfy both identification and isolation criteria.
    \end{itemize}
\end{itemize}

In each ($p_{\mathrm{T}}$, $\eta$) bin, events are further split into ``pass'' and ``fail'' categories depending on whether the probe satisfies the selection under investigation. To account for residual background contamination, a simultaneous fit is performed to the invariant mass distribution of the dilepton system in both categories. The $ Z\rightarrow\ell^+\ell^-$ signal is modelled using a superposition of Voigtian functions, with the Breit-Wigner width fixed to the natural width of the $Z$ boson. The background is described using:

\begin{itemize}
    \item A decaying exponential for the isolation and trigger efficiency fits.
    \item An error function transitioning into a decaying exponential for the ID fits. In this regime, background contamination is typically larger and must be modelled more precisely, especially in the turn-on region.
\end{itemize}

The efficiency is then extracted as:

\begin{equation_pad}
    \epsilon = \frac{N_{\text{pass}}}{N_{\text{pass}} + N_{\text{fail}}}
\end{equation_pad}

where $N_{\text{pass}}$ and $N_{\text{fail}}$ are the best-fit $Z \rightarrow \ell^+\ell^-$ signal yields in the pass and fail categories, respectively. To illustrate the procedure, Fig.~\ref{Figure:Chapter6-TagAndProbeFits_Nominal} shows an example of the invariant mass distributions for muon probes in a representative $\eta$ bin, along with the corresponding signal and background fits in the pass and fail categories. 

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/data_id_pt_20_to_25_eta_0.0_to_0.8_tpzee_nominal.pdf}
\caption{Example of the tag-and-probe fit used to extract electron identification efficiency in data. This plot corresponds to the $p_{\mathrm{T}}$ bin $20–25\GeV$ and the $|\eta|$ region 0.0–0.8.}
\label{Figure:Chapter6-TagAndProbeFits_Nominal}
\end{figure}

Efficiency \acp{SF} are computed as the ratio of data to simulation efficiencies:

\begin{equation_pad}
    \text{SF}(p_\text{T},\eta) = \frac{\epsilon_\text{data}(p_\text{T},\eta)}{\epsilon_\text{MC}(p_\text{T},\eta)}
\end{equation_pad}

These SFs are applied per object, and the total event-level weight is obtained by multiplying the per-object SFs for all relevant leptons in the event.

To assess the robustness of the extracted efficiency values and their corresponding scale factors, variations in the tag-and-probe procedure are explored. These include changes in the tag selection criteria ($\eg$ increasing the minimum $p_{\text{T}}$ threshold from $25\GeV$ to $35\GeV$), alternative signal modelling choices, and different background modelling functions. These variations result in shifts in the extracted $N_{\text{pass}}$ and $N_{\text{fail}}$ values, and consequently, in the computed efficiencies. Figure~\ref{Figure:Chapter6-TagAndProbeFits_Alternative} illustrates representative examples of these variations compared to the nominal configuration. Any residual differences are considered as systematic uncertainties in the final SF derivation.

\begin{figure}[h]
        \centering
        % First row
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/data_id_pt_20_to_25_eta_0.0_to_0.8_tpzee_signal.pdf}
            \caption{}
        \end{subfigure}
        \vspace{0.5cm}
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/data_id_pt_20_to_25_eta_0.0_to_0.8_tpzee_tightTag.pdf}
            \caption{}
        \end{subfigure}
    \caption[Impact of tag-and-probe configuration variations on efficiency extraction.]{Impact of tag-and-probe configuration variations on the measurement of electron identification efficiency in the $p_{\mathrm{T}}$ bin $20–25\GeV$ and $|\eta|$ region 0.0–0.8. Shown are invariant mass distributions and corresponding fits for: \textbf{(a)} an alternative signal model, and \textbf{(b)} a tighter tag lepton $p_\text{T}$ selection. These are compared to the nominal configuration shown in Fig.~\ref{Figure:Chapter6-TagAndProbeFits_Nominal}.}
    \label{Figure:Chapter6-TagAndProbeFits_Alternative}
\end{figure}

\subsection{\texorpdfstring{$\PGt_h$ efficiencies}{Hadronic tau efficiencies}}

\subsubsection{Identification and Energy Scale}

Efficiency SFs are applied to simulated $\tau_h$ candidates to correct for differences in reconstruction and identification performance between data and simulation. These corrections are derived centrally by the CMS collaboration and are therefore only briefly summarised here.

The SFs are obtained for $\tau_h$ objects passing the $D_\text{jet}$ discriminator using a \textit{tag-and-probe} technique in events enriched with $\PZ/\gamma^* \to \tau_\mu \tau_h$ decays. The primary observable is the visible mass of the muon and $\tau_h$ pair, $m_{\text{vis}}$, defined over the range $50 < m_{\text{vis}} < 150\GeV$. This range captures the resonance while including sidebands to help constrain backgrounds. A binned maximum-likelihood fit is performed to the $m_{\text{vis}}$ distribution, comparing data to simulation.

To improve the robustness of the extraction, a control region enriched in $\PZ/\gamma^* \to \mu\mu$ events is included simultaneously. The DY yield is treated with a common rate parameter across both categories, while the $\tau_h$ identification SF is extracted as the parameter of interest. Systematic uncertainties—including muon efficiencies, luminosity, background normalisations, and MC statistics—are incorporated into the fit as nuisance parameters.

The resulting SFs are binned in decay mode for $\pt^{\tauh} > 20\GeV$ to account for variations across kinematic regions and decay topologies. They are applied to simulated $\tau_h$ candidates matched to genuine hadronic tau decays at generator level.

In parallel, the \ac{TES} is extracted using external template fits. For each decay mode, a set of $m_{\text{vis}}$ templates is generated with varied TES values, and the best-fit value is determined via a maximum-likelihood fit. This TES is then fixed in the subsequent extraction of the $\tau_h$ identification SFs. The corrections are typically within $\pm 2\%$, consistent with the high accuracy of CMS $\tau_h$ energy reconstruction.

\subsubsection{Trigger}

Trigger efficiency corrections for $\tau_h$ candidates are measured using the same $\PZ/\gamma^* \to \tau_\mu \tau_h$ events described above, leveraging the clean topology and well-reconstructed muon leg. As with the identification and TES corrections, these measurements are performed centrally by the CMS collaboration and are only briefly summarised here.

The trigger efficiencies are extracted using a \textit{tag-and-probe} approach, where the muon serves as the tag and the $\tau_h$ as the probe. To ensure that the $\tau_h$ leg is unbiased, dedicated \textit{monitoring triggers} are used. These apply tight requirements to the muon leg while replicating the online selection of the trigger under study on the $\tau_h$ leg. This allows a direct measurement of the $\tau_h$ trigger efficiency.

Corrections are derived for both lepton+tau and double-tau triggers, and are applied to simulated events to correct for residual differences in trigger response between data and simulation.

\subsubsection{Lepton Misidentification Rates and Energy Scale}
\label{Section:Chapter6_Lepton_MisID_SF}
As discussed in Section~\ref{Section:Chapter6_Backgrounds}, electrons and muons can be misidentified as hadronic taus by the DeepTau algorithm, contributing to backgrounds in $\tau_h$ final states. The associated corrections for such misidentifications are derived centrally by the CMS collaboration and are summarised briefly here.

Misidentification probabilities are measured using a similar \textit{tag-and-probe} method in $\PZ/\gamma^* \to ee$ and $\PZ/\gamma^* \to \mu\mu$ events. One lepton serves as the tag, while the probe is required to be misreconstructed as a $\tau_h$ candidate. Events are categorised based on whether the probe passes or fails the DeepTau discriminants for electrons ($D_e$) and for muons ($D_\mu$). The misidentification rates are measured as a function of $\eta_{\tau_h}$, and the ratio of misidentification probabilities in data and simulation is extracted as the SF. These are applied to simulated misidentified $e \to \tau_h$ and $\mu \to \tau_h$ events to correct mismodelling. In addition to misidentification efficiencies, separate energy scale corrections are applied to $\tau_h$ candidates originating from misidentified leptons. 

\subsection{B-tagging efficiency}

Corrections for differences in b-tagging performance between data and simulation are applied using SFs derived centrally by the CMS collaboration. These corrections account for discrepancies in the tagging efficiency of jets originating from b-quarks, c-quarks, and light-flavour partons.

In this analysis, rather than applying a multiplicative event weight, the b-tagging status of each jet is updated on a jet-by-jet basis using the SFs. A random number generator is used to probabilistically upgrade or downgrade the tagging status of individual jets, depending on whether the SF is greater or less than one. This approach allows the modified simulated events to be treated equivalently to data in downstream selections and categorisations, eliminating the need for additional per-event weights.

% This method is beneficial for analyses where the b-tag multiplicity defines the event category, since it allows for natural migration between categories and avoids complications associated with weighted event yields. However, it is not recommended in analyses that rely on variables which could become ill-defined when upgrading untagged jets—such as those involving secondary vertex information—since upgraded jets lack associated vertex reconstruction. Care must also be taken when SFs cross unity due to systematic variations, as this may affect the stability of such variables.


\section{Background Modelling}
\label{Section:Chapter6_Background_Modelling}

As outlined in Section~\ref{Section:Chapter6_Backgrounds}, several processes can mimic the $Z^* \to \phi A \to 4\PGt$ signal signature, either through the presence of genuine $\PGt$ leptons, misidentification of jets or light leptons ($e$,$\mu$) as $\PGt_h$ candidates, or a combination of both. This section details the strategies used to model these background processes and validate their contributions in the context of the analysis. To structure the modelling, the background events are categorised into three main classes:

\begin{enumerate}[label=(\roman*)]
\item Events with only genuine $\PGt$ leptons: These irreducible backgrounds, dominated by $\PZ\PZ \to 4\PGt$ production, are modelled using simulation.

\item Events with one or more jets misidentified as $\PGt_h$ candidates (jet $\to \PGt_h$): These backgrounds are modelled using a data-driven fake factor method.

\item Events with light leptons misidentified as $\PGt_h$ candidates (lepton $\to \PGt_h$): Although typically subdominant, these backgrounds can affect some final states and are modelled using simulation with dedicated corrections, as described in Section~\ref{Section:Chapter6_Lepton_MisID_SF}.
\end{enumerate}

\subsection{\texorpdfstring{Genuine-\boldmath{$\PGt$} Backgrounds from $\PZ\PZ$}{Genuine tau Backgrounds from ZZ}}

\label{Section:Chapter6_GenuineBackground}

These events are primarily produced via quark–antiquark annihilation, with a subdominant contribution from gluon–gluon fusion. Owing to their fully leptonic final state and close kinematic resemblance to the signal, they represent the most challenging background to suppress. As summarised in Table~\ref{Table:Chapter6_SimulatedBackgrounds}, the quark-initiated component is modelled using $\POWHEG$ at NLO in QCD. At the same time, the gluon-initiated contribution is simulated at LO using $\PYTHIA$.  To account for missing higher-order corrections, both components are scaled using multiplicative $\mathcal{K}$-factors~\cite{Kfactors_ZZ} as a function of the four-lepton invariant mass $m_{4\ell}$.

\begin{itemize}
\item Quark-initiated $\PZ\PZ$: NLO-to-NNLO QCD $K$-factors are determined using $\POWHEG$ 2.0~\cite{Powheg_1,Powheg_2}. These corrections are applied as a function of $m_{4\ell}$, with an average $K$-factor of approximately 1.2 across the relevant phase space.

\item Gluon-initiated $\PZ\PZ$: A significantly larger $K$-factor, typically around 2.0, is applied to match NNLO QCD predictions derived from \textsc{HNNLO v2}~\cite{PhysRevLett.98.222002}. The correction is used as a function of $m_{4\ell}$, in the same way as for the quark-initiated component.
\end{itemize}

To ensure accurate modelling of this background, the simulation is validated in a control region enriched in $\PZ\PZ \to \mu^+\mu^-\mu^+\mu^-$ decays. This channel features a clean experimental signature with minimal contamination from other backgrounds. The same selection criteria outlined in Table~\ref{Table:Chapter6_ObjectSelectionSummary} are applied, with minor adaptations to increase statistical precision. Specifically, the muon isolation threshold is relaxed to $I^{\mu}_\text{PF} < 0.35$, while the overall charge of the final state is required to be zero. To complete the definition of the control region, the trigger configuration used to select events in this region is summarised in Table~\ref{Table:Chapter6_TriggersPerChannel}. Figure~\ref{Figure:Chapter6_ZZ_KfactorImpact} illustrates the effect of applying the $\mathcal{K}$-factors on the $\PZ\PZ$ background prediction, showing improved agreement with data in the $\mu^+\mu^-\mu^+\mu^-$ invariant mass distribution.

\begin{figure}[h]
        \centering
        % First row
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mmmm_wo_kfactors.pdf}
            \caption{}
        \end{subfigure}
        \vspace{0.5cm}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mmmm_with_kfactors.pdf}
            \caption{}
        \end{subfigure}
    \caption[Impact of $\mathcal{K}$-factors on the $\PZ\PZ$ background prediction in the four-muon control region.]{Impact of $\mathcal{K}$-factors on the $\PZ\PZ$ background prediction in the four-muon control region. Shown are the invariant mass distributions of the $\PGm^+\PGm^-\PGm^+\PGm^-$ system before (\textbf{a}) and after (\textbf{b}) applying $\mathcal{K}$-factors.}
    \label{Figure:Chapter6_ZZ_KfactorImpact}
\end{figure}

Although the $\PZ\PZ$ background is considered irreducible by definition, it can still be effectively suppressed through targeted offline selections that exploit subtle kinematic differences between the signal and background. For instance, in the $\PGt_e\PGt_h\PGt_h\PGt_h$ final state, the signal typically displays a harder $p_{\mathrm{T}}$ spectrum for the subleading leptons compared to the background. The object selection criteria applied in this analysis take advantage of this distinction. Figure~\ref{Figure:Chapter6_ThirdLepPt} shows the $p_{\mathrm{T}}$ distribution of the third lepton in this channel, demonstrating how the applied cuts significantly reduce the $\PZ\PZ$ background while retaining the majority of the signal.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter6/ZZ_OfflineCutImpact.pdf}
    \caption[$p_{\mathrm{T}}$ spectrum of the third lepton in the $\PGt_e\PGt_h\PGt_h\PGt_h$ channel.]{Distribution of the transverse momentum of the third lepton in the $\PGt_e\PGt_h\PGt_h\PGt_h$ final state, comparing the expected $\PZ\PZ$ background to the signal prediction. The solid line at $20\GeV$ region indicates the $p_{\mathrm{T}}$ selection threshold applied.}
    \label{Figure:Chapter6_ThirdLepPt}
\end{figure}

\subsection{\texorpdfstring{Background from Jets Misidentified as $\PGt_h$ (Jet $\to \PGt_h$)}{Background from Jets Misidentified as hadronic taus}}

\label{Section:Chapter6_JetToTauBackground}

This section discusses how the background from jets misidentified as $\PGt_h$ candidates is modelled. These backgrounds arise when jets from processes such as the ones outlined in Section~\ref{Section:Chapter6_Backgrounds} are misidentified as hadronic tau decays. Such misidentifications are particularly problematic to model in simulation, as the jet-to-$\PGt_h$ fake rate is poorly described in MC simulations. Moreover, the small probability of a jet being misidentified as a $\PGt_h$ candidate makes it necessary to generate high-statistics MC samples, which comes with a significant computational expense. These limitations and the difficulty in capturing the full range of misidentification scenarios in simulation motivate the use of a data-driven approach, such as the Fake Factor ($\mathcal{F_F}$) method.

\subsubsection{Classical Fake Factor Method}
\label{Section:Chapter6_FakeFactors_Classical}

This classical $\mathcal{F_F}$ method begins with the definition of a \textbf{\ac{DR}}, which is a region in data enriched with jet-to-$\PGt_h$ events. The DR is purified by subtracting any non-jet$\to\PGt_h$ events using simulation. This ensures that the region predominantly contains events where jets are misidentified as $\PGt_h$. Once the DR is established, the $\mathcal{F_F}$ is calculated as the ratio of events that \textit{pass} the \texttt{Nominal} tau identification requirement to those that \textit{fail} it but pass a looser \texttt{Alternative} tau identification requirement.

\begin{equation}
\mathcal{F_F} = \frac{\mathcal{N}(\texttt{Nominal})}{\mathcal{N}(\texttt{Alternative and not Nominal})}
\end{equation}

The choice of the alternative identification requirement introduces a trade-off. Looser identification thresholds reduce the statistical uncertainty by increasing the number of events in the denominator. However, this comes at the cost of increased systematic uncertainty, as the looser $\PGt_h$ candidates are less representative of those in the SR. 

The $\mathcal{F_F}$ is computed differentially in key variables such as $p_\text{T}$, jet multiplicity ($\mathcal{N}_{\text{jets}}$), and other observables that influence the misidentification rate. Furthermore, because the $\mathcal{F_F}$ can depend on the choice of variable defining the DR, additional corrections are often derived using sideband regions. These corrections aim to account for residual dependencies and improve the robustness of the background estimation.

Once derived, the Fake Factor is applied to the \ac{AR}, which is constructed to resemble the SR. Specifically, the AR requires the jet-to-$\PGt_h$ events to pass the \texttt{Alternative} requirement but to fail the \texttt{Nominal} requirement. This allows for a data-driven estimate of the jet-to-$\PGt_h$ background contribution in the SR. 

While the classic FF method is simple and robust, it has notable drawbacks:
\begin{itemize}
\item \textit{Curse of dimensionality}: Adding more variables exponentially increases the number of bins needed to capture correlations, rapidly depleting available statistics. This leads to sparsely populated bins, large statistical fluctuations, and unstable $\mathcal{F_F}$ values that can compromise the reliability of the background estimate.

\item The limited dimensionality of the parameterisation means that correlations with variables not explicitly included in the fit are not accounted for. As a result, these unfitted variables may be poorly modelled, and corrections derived from control or sideband regions are often needed. However, such corrections can improve agreement in one set of variables at the expense of others, highlighting the limitations of low-dimensional tuning.

\item In final states with multiple $\PGt_h$ candidates, such as $ \PGt_h \PGt_h \PGt_h \PGt_h $, the $\mathcal{F_F}$ method becomes increasingly cumbersome. A separate $\mathcal{F_F}$ must be derived and applied for each $\PGt_h$ candidate, typically starting from the leading one and sequentially correcting the subleading ones. This iterative procedure amplifies the statistical challenges, particularly in low-yield regimes, and complicates the background estimation.
\end{itemize}

This multi-$\PGt_h$ final state problem is further illustrated by previous CMS analyses~\cite{CMS:2022goy,Mb:2022rxu}, where the $\mathcal{F_F}$ was typically derived only for the leading $\PGt_h$ candidate. Events where the leading object was a genuine $\tau$ were corrected using simulation. This simplification was acceptable in those studies, as events with one genuine and one misidentified $\PGt_h$ candidate were subdominant. In contrast, this analysis features final states with two or more misidentified $\PGt_h$ candidates, making this assumption invalid. A more general and flexible approach is therefore required to consistently account for all $\PGt_h$ candidates.

\subsubsection{Machine Learning-based reweighting}
\label{Section:Chapter6_FakeFactors_BDT}

To overcome the limitations of the classical binning-based approach, \ac{ML}-based density ratio estimation can be employed. The goal is to estimate the misidentification rate as a function of multiple observables, which mathematically corresponds to estimating the ratio of probability densities $f_{\text{pass}}(x)/f_{\text{fail}}(x) $. This can be achieved using \ac{ML} techniques such as \acp{BDT}, which provide class probabilities that approximate this density ratio:

\begin{equation_pad}
\frac{f_{\text{pass}}(x)}{f_{\text{fail}}(x)} \sim \frac{p_{\text{pass}}(x)}{p_{\text{fail}}(x)}.
\end{equation_pad}

However, classical classification algorithms can struggle in regions where the density ratio is large. In these cases, the classification task becomes trivial because most events belong to a single class. Consequently, the model focuses its learning on regions where the classes are more balanced and harder to distinguish. High-density-ratio regions contribute little to the training objective\footnote{The standard loss function is cross-entropy, which assigns lower weight to well-separated (\ie, high-ratio) regions due to their ease of classification.}, resulting in less accurate predictions in precisely the regions where reliable reweighting is most needed for the $\mathcal{F_F}$ method.

To address this, a custom objective function is introduced to target discrepancies between the pass and fail distributions. In tree-based models, this function is optimised over sets of events grouped into \textit{leaves}\footnote{Leaves are the terminal nodes of a decision tree; each leaf contains events satisfying the same set of decision rules.}. Rather than focusing on balanced regions, the objective prioritises leaves where the difference between the two distributions is largest. In this context, the \textit{symmetrised} $\chi^2$ metric is used:

\begin{equation_pad}
    \chi^2 = \sum_{\text{leaf}} \frac{(w_{\text{leaf}, \text{fail}} - w_{\text{leaf}, \text{pass}})^2}{w_{\text{leaf}, \text{fail}} + w_{\text{leaf}, \text{pass}}}
\end{equation_pad}

where $w_{\text{leaf}, \text{fail}}$ and $w_{\text{leaf}, \text{pass}}$ denote the total event weights from the fail and pass samples, respectively, in each leaf. By maximising this objective during training, the classifier is encouraged to concentrate on leaves with the largest discrepancies, which are the most critical for improving the accuracy of the reweighting.

The training proceeds in a boosting-like fashion, where multiple shallow decision trees are trained sequentially. Each tree focuses on correcting discrepancies not addressed by the previous ones. At each iteration, the following steps are performed:

\begin{itemize}
    \item A shallow decision tree that maximises the symmetrised $\chi^2$ is built.
    \item For each leaf, the log-ratio of total weights is computed:
    \begin{equation_pad}
        r_\text{leaf} = \log \left( \frac{w_{\text{leaf}, \text{fail}}}{w_{\text{leaf}, \text{pass}}} \right)
    \end{equation_pad}
    \item In each leaf, the events of the fail distribution are reweighted by multiplying by $w = w \times e^{r{_\text{leaf}}}$.
\end{itemize}

This procedure is repeated over multiple iterations, allowing each tree to iteratively refine the reweighting by focusing on the residual differences between the distributions.

\subsubsection{Parameterisation and application}

Having described the machine learning-based reweighting approach conceptually, this section presents its implementation in the context of this analysis. This includes the definition of the fitting regions, the choice of input variables used to train the classifier, and the procedure for applying the resulting weights to estimate the jet $\to~\PGt_h$ background in the signal region. 

As introduced in Section~\ref{Section:Chapter6_FakeFactors_Classical}, the classical $\mathcal{F_F}$ method relies on defining a set of control regions. Figure~\ref{Figure:Chapter6_ABCD} illustrates how these regions can be organised into a traditional ABCD-style framework, with sideband selections based on variables such as tau identification and total charge.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter6/ABCD.pdf}
    \caption[ABCD-style region definition for fake factor estimation.]{Schematic illustration of an ABCD-style region definition used in the fake factor method.}
    \label{Figure:Chapter6_ABCD}
\end{figure}

The sideband selections used to define the ABCD regions vary by channel and are summarised below:

\begin{itemize}
    \item \textbf{For the $\Pe\Pe\PGt_h\PGt_h$, $\Pe\Pgm\PGt_h\PGt_h$, $\Pgm\Pgm\PGt_h\PGt_h$, $\Pe\PGt_h\PGt_h\PGt_h$, and $\Pgm\PGt_h\PGt_h\PGt_h$ channels:}
    \begin{itemize}
        \item \textit{y-axis (charge category):}
        \begin{itemize}
            \item Region C: $\sum q_{\PGt_h} = 0$
            \item Region A: $\sum q_{\PGt_h} \ne 0$
        \end{itemize}
        \item \textit{x-axis (tau ID category):}
        \begin{itemize}
            \item Region C: All alternative $\PGt_h$ candidates pass the Loose DeepTauVsJets working point.
            \item Region D: At least one alternative $\PGt_h$ candidate fails the Loose working point but has raw DeepTau score $> 0.1$.
        \end{itemize}
    \end{itemize}
    
    \item \textbf{For the $\PGt_h\PGt_h\PGt_h\PGt_h$ channel:}
    \begin{itemize}
        \item \textit{y-axis (charge category):}
        \begin{itemize}
            \item Region C: $|\sum q_{\PGt_h}| = 1$
            \item Region A: $|\sum q_{\PGt_h}| \ne 1$
        \end{itemize}
        \item \textit{x-axis:} Same as above, based on Loose WP and DeepTau score threshold.
    \end{itemize}
\end{itemize}


While the ABCD schematic in Figure~\ref{Figure:Chapter6_ABCD} appears straightforward when applied to a single $\PGt_h$ candidate, it becomes significantly more complex in events with multiple candidates. In classical applications, fake factors are typically derived separately for each candidate—most commonly the leading one—by applying nominal and alternative identification criteria in sideband-defined regions. However, when more than one $\PGt_h$ candidate is present, this necessitates constructing a separate ABCD diagram for each, dramatically increasing the dimensionality and fragmenting the data into many statistically limited subsets.

The machine learning-based approach overcomes this limitation by removing the need to define ABCD regions explicitly. Instead, each $\PGt_h$ candidate is treated as a separate training instance, contributing an individual row to the training dataset. Candidate-level and event-level input features—including those that would traditionally define ABCD axes, such as identification status and sideband classifications—are passed directly to the model. This allows the reweighting function to be learned over the entire parameter space without the need to construct or populate distinct ABCD regions. As a result, all candidates across all events are used in a single high-statistics training procedure, enabling flexible and comprehensive modelling of the jet $\to \PGt_h$ fake rate. The variables used in this parameterisation are summarised in Table~\ref{Table:Chapter6_InputVars}.

While in the classical fake factor method (Section~\ref{Section:Chapter6_FakeFactors_Classical}), backgrounds from non-jet$\to\PGt_h$ candidates are removed by subtracting simulation from data at the histogram level. Generator-matched MC is used to identify and subtract these non-target contributions, resulting in a hybrid histogram enriched in genuine jet$\to\PGt_h$ events. However, this approach is not feasible in the machine learning-based framework, which operates on full datasets rather than histograms and does not support negative event weights.

One possible solution would be template matching: for each entry in data, a closely matching non-jet$\to\PGt_h$ MC event could be identified and removed to mimic histogram subtraction at the event level. However, this method is computationally prohibitive given the high dimensionality of the input space and the large size of the datasets.

Instead, a custom binary BDT classifier is trained on simulation to distinguish jet$\to\PGt_h$ from non-jet$\to\PGt_h$ candidates, using the same input variables as the reweighter. This reduces the problem to a single BDT score, providing a one-dimensional representation of how "non-jet$\to\PGt_h$-like" each candidate is. A histogram of these scores is built from the non-jet$\to\PGt_h$ MC and scaled to match the expected cross section. The same BDT is then applied to the data. For each bin of the MC histogram, candidates in data with matching BDT scores are randomly removed until the predicted contamination is accounted for.

This procedure results in a purified dataset of jet$\to\PGt_h$-like candidates that closely mimics the outcome of histogram-level subtraction, but in a form compatible with the machine learning-based reweighting method.

\section{Search strategy and statistical procedure}







