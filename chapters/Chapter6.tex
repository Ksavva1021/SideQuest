\chapter{\texorpdfstring{Search for extended Higgs sector signatures in $\PGt^+\PGt^-\PGt^+\PGt^-$ final states}{Search for extended Higgs sector signatures in tautautautau final states}}
\chaptermark{Extended Higgs sector search}  
\thispagestyle{plain}  % First page has default style
\pagestyle{chapterpages}
\label{Section:Chapter_4tau}
\minitoc

\section{Introduction}

Motivated by the theoretical considerations and the particularly intriguing measurement of the muon anomalous magnetic moment discussed in Section~\ref{Section:Chapter2_gminus2}, this chapter presents a detailed analysis targeting final states with four tau leptons through the process $Z^*\rightarrow\phi A\rightarrow4\PGt$. The analysis explores a region of parameter space that remains largely unconstrained by existing collider searches. This is primarily because the production mode proceeds via an off-shell $Z^*$ boson. This circumvents the dominant SM Higgs production mechanisms, which are already tightly constrained by current LHC measurements. As such, this search offers unique sensitivity to scenarios in extended Higgs sectors that could otherwise evade detection.

This chapter provides a comprehensive description of the analysis strategy employed in the CMS experiment to probe this signature. 

\section{Data and Simulation}
\subsection{Collision data}

This search is based on pp collision data collected by the CMS detector during the 2016-2018 Run 2 data-taking period. The collisions were recorded at a centre-of-mass energy $\sqrt{s} = 13\TeV$ and the full dataset corresponds to an integrated luminosity of approximately $138\unit{fb}^{-1}$.

\subsection{Backgrounds}
\label{Section:Chapter6_Backgrounds}
This section provides a brief overview of the SM processes that can contribute to the four-$\PGt$ signal region. The aim is to provide an understanding of how different backgrounds can enter the selection through the presence of genuine or misidentified tau leptons. The specific treatment of each background category is described in detail in Section~\ref{Section:Chapter6_Background_Modelling}.

The \textbf{\ac{DY}} process, $\PZ/\gamma^* \to \ell^+\ell^-$, is one of the dominant backgrounds in ditau final states, producing genuine $\PGt^+\PGt^-$ pairs in approximately one-third of events. Although its contribution is reduced in four-tau final states, it remains relevant through three distinct mechanisms:

\begin{enumerate}[label=(\roman*)]
\item Events with genuine $\PZ/\gamma^* \to \PGt^+\PGt^-$ decays accompanied by jets from ISR/FSR can pass the four-tau selection if those jets are misidentified as hadronic tau candidates.\footnote{Depending on how the process is generated, ISR/FSR jets (\ie jets arising from QCD radiation) may originate from the parton shower or be included at matrix-element level.}

\item In events where the boson decays to $\Pe^+\Pe^-$ or $\PGm^+\PGm^-$, the prompt electrons or muons can mimic nonprompt leptons from tau decays. When accompanied by additional jets misidentified as hadronic tau candidates, such events may satisfy the four-tau selection despite containing no genuine tau leptons.

\item Prompt electrons or muons from DY decays can also be directly misidentified as hadronic tau candidates. Similar to the case above, when additional jets are misidentified as hadronic taus, these events can satisfy the four-tau selection without involving any genuine tau decays.
\end{enumerate}

Representative Feynman diagrams illustrating DY production with and without ISR/FSR are shown in Fig.~\ref{Figure:Chapter6_DY}.

\begin{figure}[!htbp]
    \centering
    % First row
    \begin{subfigure}{0.45\textwidth}
        \centering
        \input{FeynmanDiagrams/DY.tex}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.45\textwidth}
        \centering
        \input{FeynmanDiagrams/DY_ISR.tex}
        \caption{}
    \end{subfigure}

    \caption[Examples of Feynman diagrams for Drell-Yan without partons and one parton originating from initial state radiation.]{Examples of Feynman diagrams for Drell-Yan \textbf{(a)} without partons and \textbf{(b)} one parton originating from ISR.}
    \label{Figure:Chapter6_DY}
\end{figure}

\textbf{Diboson} production, specifically $\PZ\PZ \to 4\PGt$, constitutes the primary irreducible background in this analysis. These events contain four genuine tau leptons in the final state, closely resembling the signal topology by construction. Although the cross section is small compared to other SM processes, their kinematic features make them difficult to distinguish from the signal. Additional background arises from $\PZ\PZ$ events in which one or both $\PZ$ bosons decay to electrons or muons instead of tau leptons, resulting in mixed-flavour final states such as $e^+e^-\PGt^+\PGt^-$ or $\mu^+\mu^-\PGt^+\PGt^-$. These can enter the signal region via the same lepton misidentification pathways discussed for DY.

\textbf{Top quark pair production} contributes to the background when both $\PW$ bosons decay leptonically via $\PW \to \PGt \, \nu_\PGt$, yielding two genuine tau leptons in the final state. The accompanying $b$ jets, as well as additional jets from QCD radiation, can be misidentified as hadronic tau candidates, leading to events that satisfy the four-tau selection despite containing only two genuine tau leptons. In addition, as in the DY background, prompt electrons or muons from $\PW \to e\,\nu_e$ or $\PW \to \mu\,\nu_\mu$ decays may be misidentified as leptonic or hadronic tau candidates. When combined with misidentified jets or reconstructed tau decay modes, these events may satisfy the four-tau selection despite containing no genuine tau leptons.

\textbf{W+jets} events can enter the four-tau signal region through mechanisms similar to those described for top quark pair production, but with only a single $\PW$ boson in the final state. When the $\PW$ decays via $\PW \to \PGt \, \nu_\PGt$, the event contains one genuine tau lepton. Accompanying jets from QCD radiation may be misidentified as hadronic tau candidates, allowing the event to mimic a multi-tau topology. In addition, prompt electrons or muons from $\PW \to e\,\nu_e$ or $\PW \to \mu\,\nu_\mu$ decays can be misidentified either as hadronic tau candidates or as leptonic tau decays, again mirroring the behaviour observed in the $\ttbar$ background. Since W+jets events typically contain only one genuine light lepton or tau, multiple misidentifications of jets or leptons are required for the event to satisfy the four-tau selection.

\textbf{QCD-induced multijet} events can enter the four-tau signal region through multiple misidentification pathways. These events contain no genuine tau leptons, but may satisfy the selection criteria when several jets are misidentified as hadronic tau candidates. In addition, nonprompt electrons or muons, primarily from semileptonic decays of heavy-flavour hadrons or in-flight decays of light mesons, can also be misidentified as leptonic or hadronic tau candidates.

\textbf{Diboson} processes such as $\PW\PW$ and $\PW\PZ$ can contribute to the four-tau signal region when one or more of the bosons decay leptonically via $\PW \to \PGt \, \nu_\PGt$ or $\PZ \to \PGt^+\PGt^-$, producing up to three genuine tau leptons in the final state. The remaining candidates typically arise from jet or lepton misidentification, as in the DY, $\ttbar$, and W+jets backgrounds.

\textbf{Triboson} production (e.g., $\PW\PW\PZ$, $\PZ\PZ\PZ$) can yield three or more prompt leptons, including genuine tau decays. Such events may enter the signal region through analogous combinations of genuine and misidentified tau candidates. Other processes, such as single-top production and electroweak $\PZ/\PW$+jets (\eg VBF-like topologies), can also contribute via similar mechanisms but are subdominant.

Although many of the backgrounds discussed above are partially reducible through the application of tau identification algorithms, lepton isolation requirements, and $b$-jet vetoes, these techniques are not fully efficient, and some backgrounds remain irreducible. A common feature across many of these processes is the presence of prompt electrons or muons that can be reconstructed as leptonic or hadronic tau candidates. When combined with jet misidentification, this allows events with fewer than four genuine tau leptons to satisfy the full selection criteria. The specific treatment, estimation, and validation of each background category are described in Section~\ref{Section:Chapter6_Background_Modelling}.

The simulated background processes used in this analysis are summarised in Table~\ref{Table:Chapter6_SimulatedBackgrounds}.

{
\centering
\setlength{\LTpost}{-2ex}  % tighten space after table
\small  % one size smaller than normal
\begin{longtable}{llc}
\caption[Summary of the simulated Standard Model backgrounds, including their generators and precision, used in the extended Higgs sector search.]
{Summary of the simulated SM backgrounds, including their generators and precision, used in the search. The following generators were used: 
\MADGRAPH~\cite{MadGraph} for leading-order matrix element calculations; 
\POWHEG~v1.0~\cite{Powheg_0} and v2.0~\cite{Powheg_1,Powheg_2,Powheg_3} for next-to-leading order processes including $\ttbar$ and single top; 
and \MGvATNLO~\cite{MadGraph} for diboson and triboson production. 
Parton showering and hadronisation were performed with \PYTHIA~\cite{PYTHIA}.}
\label{Table:Chapter6_SimulatedBackgrounds} \\
\hline
\textbf{Process} & \textbf{Generators} & \textbf{Cross section $\sigma$ [pb]} \\
\hline \hline
\endfirsthead

\hline
Process & Generators & Cross section $\sigma$ [pb] \\
\hline \hline
\endhead

\hline
\multicolumn{3}{r}{\textit{Continued on next page}} \\
\endfoot

\hline
\endlastfoot
\rowcolor{verylightblue}
\textbf{Drell-Yan, $\PZ/\gamma^* \to \ell^+ \ell^-$ (LO)\hyperlink{DY_W-MLM}{$^1$}} & & \\
+ jets, $10 < m_{\ell \ell} < 50\GeV$ & \MADGRAPH, \PYTHIA & 15810.0 (LO), 18610.0 (NLO) \\
+ jets, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 5379.0 (LO), 6077.2 (NNLO) \\
+1 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 997.3 (LO) \\
+2 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 347.0 (LO)\\
+3 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 126.4 (LO) \\
+4 jets\hyperlink{DY_W-Stitch}{$^2$}, $m_{\ell \ell} > 50\GeV$ & \MADGRAPH, \PYTHIA & 71.7 (LO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{W+jets (LO)\hyperlink{DY_W-MLM}{$^1$}} & & \\
+ jets & \MADGRAPH, \PYTHIA & 52940.0 (LO), 61526.7 (NLO) \\
+1 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 9364.4 (LO) \\
+2 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 3168.6 (LO) \\
+3 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 1132.1 (LO) \\
+4 jets\hyperlink{DY_W-Stitch}{$^2$} & \MADGRAPH, \PYTHIA & 633.7 (LO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{\ttbar} & & \\
Fully hadronic & \POWHEG, \PYTHIA & 377.96 (NNLO)\\
Semi-leptonic & \POWHEG, \PYTHIA & 365.34 (NNLO)\\
Fully leptonic & \POWHEG, \PYTHIA & 88.29 (NNLO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Single top} & & \\
t-channel ($t$) & \POWHEG, \PYTHIA & 136.02 (NNLO) \\
t-channel ($\overline{t}$) & \POWHEG, \PYTHIA & 136.02 (NNLO) \\
$t + W^-$ & \POWHEG, \PYTHIA & 35.60 (NNLO) \\
$t + W^+$ & \POWHEG, \PYTHIA & 35.60 (NNLO) \\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Diboson} & & \\
$\PW \PZ \rightarrow \ell \, \nu \, \nu \, \nu$  & \MCATNLO, \PYTHIA & 3.416 (NLO) \\
$\PW \PZ \rightarrow \ell \, \nu \, q \, q$        & \MCATNLO, \PYTHIA & 10.71 (NLO) \\
$\PW \PZ \rightarrow q \, q \, \ell \, \ell$            & \MCATNLO, \PYTHIA & 6.419 (NLO) \\
$\PW \PZ \rightarrow \ell \, \ell \,  \ell \, \nu $          & \MCATNLO, \PYTHIA & 5.213 (NLO) \\
$\PW \PW \rightarrow \ell \, \nu \, q \,q$        & \MCATNLO, \PYTHIA & 49.99 (NLO) \\
$\PW \PW \rightarrow \ell \, \nu \, \ell \, \nu$        & \POWHEG, \PYTHIA & 11.09 (NLO) \\
$\PZ \PZ \rightarrow \ell \, \ell \, \nu \, \nu$         & \POWHEG, \PYTHIA & 0.9740 (NLO) \\
\arrayrulecolor{lightgray}\hline
$\text{H}_{\text{VBF}} \rightarrow ZZ \rightarrow \ell \, \ell \, \ell \, \ell $ & \POWHEG, \PYTHIA & $1.040e^{-3}$ (NLO) \\
$\text{ggH} \rightarrow ZZ \rightarrow \ell \, \ell \, \ell \, \ell $ & \POWHEG, \PYTHIA & $1.333e^{-2}$ (NLO)\\
$qq \rightarrow ZZ \rightarrow \ell \, \ell \, \ell \, \ell $ & \POWHEG, \PYTHIA & 1.325 (NLO)\\
$gg \rightarrow ZZ \rightarrow e \, e \, \PGt \, \PGt $ & \PYTHIA & $3.194e^{-3}$ (NLO)\\
$gg \rightarrow ZZ \rightarrow \mu \, \mu \, \PGt \, \PGt $ & \PYTHIA & $3.194e^{-3}$ (NLO)\\
$gg \rightarrow ZZ \rightarrow \mu \, \mu \, \mu \, \mu $ & \PYTHIA & $1.585e^{-3}$ (NLO)\\
$gg \rightarrow ZZ \rightarrow \PGt \, \PGt \, \PGt \, \PGt $ & \PYTHIA & $1.585e^{-3}$ (NLO)\\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Triboson} & & \\
$\PW \PW \PZ $ & \MCATNLO, \PYTHIA & 0.1707 (NLO)\\
$\PW \PZ \PZ $ & \MCATNLO, \PYTHIA & 0.0571 (NLO)\\
$\PW \PW \PW $ & \MCATNLO, \PYTHIA & 0.2158 (NLO)\\
$\PZ \PZ \PZ $ & \MCATNLO, \PYTHIA & 0.0148 (NLO)\\

\arrayrulecolor{lightgray}\hline
\rowcolor{verylightblue}
\textbf{Pure Electroweak} & & \\
$\PW^+ \to \ell^+ \nu$ + 2 jets & \MADGRAPH, \PYTHIA & 25.62 (LO)\\
$\PW^- \to \ell^- \nu$ + 2 jets & \MADGRAPH, \PYTHIA & 20.25 (LO)\\
$\PZ \to \ell \ell$ + 2 jets & \MADGRAPH, \PYTHIA & 3.987 (LO)\\

\arrayrulecolor{black}\hline
\end{longtable}
}
\vspace{0.5em}
\noindent\begin{minipage}{\linewidth}
\footnotesize
\hypertarget{DY_W-MLM}{}$^{1}$In the DY+jets simulation, the MLM jet matching scheme~\cite{MLM} is employed to consistently combine partons generated at the matrix-element level with those from the parton shower, avoiding double counting of ISR/FSR jets. \\

\hypertarget{DY_W-Stitch}{}$^{2}$ To improve statistical precision, exclusive samples with fixed jet multiplicities are generated using the MLM scheme. These are stitched with inclusive samples to reproduce the overall cross-section and kinematic distributions accurately.
\end{minipage}

% For \ac{NLO} simulations, however, the MLM scheme is incompatible with the negative event weights that naturally arise. In such cases, the FxFx jet merging scheme~\cite{FxFx} is used instead. 


% Although the background samples are generated at LO or NLO, the comparison of simulated events to data is performed using higher-order theoretical cross-sections. Specifically, $\PW$+jets, Z+jets, $\ttbar$, and single top quark events in the tW channel are normalised using \ac{NNLO} cross-sections~\cite{HighOrder_XS_1,HighOrder_XS_2,HighOrder_XS_3}, while single top and diboson events are normalised to cross-sections calculated at NLO~\cite{HighOrder_XS_3,HighOrder_XS_4,HighOrder_XS_5}.

\subsection{Modelling of Signal Processes}
\label{Section:Chapter6_SignalModelling}

Signal templates targeting the process $Z^* \to \phi A \to \PGt^+\PGt^-\PGt^+\PGt^-$ are modelled in the \MCATNLO (version 2.6.5) framework~\cite{MadGraph,FxFx} at NLO accuracy in QCD. Event generation is performed in the five-flavour scheme using the NNPDF3.1 PDFs~\cite{NNPDF}. Parton showering, hadronisation, and tau lepton decays are simulated using \PYTHIA (version 8.230)~\cite{PYTHIA}, with the underlying event modelled according to the CP5 tune~\cite{CP5_Tune}. To reflect realistic running conditions, simulated events are overlaid with additional pp interactions according to the PU distribution observed during the Run~2 data-taking period. The detector response is simulated using the \GEANTfour-based CMS software~\cite{GEANT4}, and events are reconstructed with the same configuration as applied to collision data.

A two-dimensional mass grid is defined to ensure sensitivity across the allowed phase space. All masses are expressed in units of~\GeV. The grid consists of combinations of $m_A$ and $m_\phi$ as summarised in Table~\ref{Table:Chapter6_4tauMassGrid}.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{12pt} % Increase column width
\arrayrulecolor{black} % Ensure outer border is black
\begin{tabular}{cc}
\hline
$m_A$ & $m_\phi$ \\
\arrayrulecolor{black} \hline

40--90     & 60, 70, 80, 90, 100, 110, 125, 140, 160, 180, 200, 250, 300 \\
\arrayrulecolor{lightgray} \hline

100--250   & Above \& 400 \\
\arrayrulecolor{lightgray} \hline

300        & Above \& 600 \\
\arrayrulecolor{lightgray} \hline

400        & 100, 110, 125, 140, 160, 180, 200, 250, 300, 400, 600 \\
\arrayrulecolor{lightgray} \hline

600        & 400, 600, 800 \\
\arrayrulecolor{black} \hline

\end{tabular}
\caption{Mass points used in signal generation. For each $m_A$ value, the corresponding $m_\phi$ values are listed. All masses are in GeV.}
\label{Table:Chapter6_4tauMassGrid}
\end{table}

The signal is simulated under the \textit{alignment limit} of the 2HDM, consistent with current experimental constraints on the couplings of the observed Higgs boson. The specific scenario is dynamically selected at each mass point depending on the value of $m_\phi$:
\begin{enumerate}[label=(\roman*)]
    \item For $m_\phi > 125~\GeV$, the heavier CP-even scalar is identified as $\phi \equiv H$
    \item For $m_\phi \leq 125~\GeV$, the lighter CP-even scalar is identified as $\phi \equiv h$
\end{enumerate}

Model parameters that do not significantly influence the kinematic properties of the final state, such as the charged Higgs mass and the soft $\mathbb{Z}_2$-breaking term, are fixed to values that preserve perturbative unitarity. This also ensures theoretical consistency~\cite{TypeX_2HDM}. Variations of these parameters within their allowed ranges have a negligible impact on the resulting signal kinematics. Therefore, these parameters are set according to the following prescription:
\begin{equation_pad}
m_{H^\pm} = m_\phi, \quad\quad m_{12}^2 = m_\phi^2 \sin\beta \cos\beta.
\end{equation_pad}

Figure~\ref{Figure:Chapter6_GenVisDistributions} illustrates the generator-level distributions of the visible di-$\PGt$ mass for selected combinations of $m_\phi$ and $m_A$.The visible mass is defined as the invariant mass of the visible decay products of the two $\PGt$ leptons, excluding neutrinos. These distributions provide a representative overview of signal kinematics across the mass grid.

\begin{figure}[!htbp]
        \centering
        % First row
        \begin{subfigure}[b]{0.7\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mvis_A.pdf}
            \caption{}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.7\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mvis_phi.pdf}
            \caption{}
        \end{subfigure}
    \caption[Generator-level visible mass distributions for different mass combinations of $\phi$ and A.]{Generator-level distributions of the visible di-$\PGt$ mass for selected combinations of $m_A$ and $m_\phi$. The visible mass is computed using only the decay products of the two $\PGt$ leptons, excluding neutrinos. Distributions are shown for scans over \textbf{(a)} $m_A$ and \textbf{(b)} $m_\phi$.}
    \label{Figure:Chapter6_GenVisDistributions}
\end{figure}

\subsubsection{Production Cross Sections and Branching Fractions}

The production cross section is computed at NLO in QCD and, within the alignment limit, is independent of $\tan\beta$. It varies across the mass plane, ranging from approximately $10~\unit{fb}$ at $(m_\phi, m_A) = (100, 60)~\GeV$ to $650~\unit{fb}$ at $(300, 160)~\GeV$. These values are summarised in Figure~\ref{Figure:Chapter6_ProductionXS}, which shows the cross section as a function of the two mass parameters in the alignment scenario.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{Figures/Chapter6/Production_XS.pdf}
    \caption[Production cross sections for $Z^* \to \phi A$ in the alignment limit.]{Production cross sections for $Z^* \to \phi A$ in the alignment limit at NLO, shown as a function of $m_\phi$ and $m_A$. The values are interpolated over a grid of simulated mass combinations.}
  \label{Figure:Chapter6_ProductionXS}
\end{figure}

Outside the alignment limit, the cross section acquires dependence on the scalar mixing angle. Specifically:
\begin{itemize}
    \item It scales with $\sin^2(\beta - \alpha)$ when $\phi \equiv H$ ($m_\phi > 125~\GeV$),
    \item It scales with $\cos^2(\beta - \alpha)$ when $\phi \equiv h$ ($m_\phi \leq 125~\GeV$).
\end{itemize}

The decay branching ratios of $\phi$ and $A$ into $\PGt^+\PGt^-$ are computed using \textsc{2HDECAY}~\cite{2HDECAY}, a dedicated tool for 2HDM decay widths and branching ratios. In the alignment limit, the $\PGt^+\PGt^-$ branching ratio of $A$ is close to unity for $\tan\beta \gtrsim 2$, but falls sharply at lower values, where hadronic decays such as $A \to b\bar{b}$ become dominant. A similar pattern holds for $\phi \to \PGt^+\PGt^-$, except when the mass difference $m_\phi - m_A$ exceeds $m_Z$. In such cases, the decay $\phi \to ZA$ becomes kinematically accessible and can significantly suppress the di-$\PGt$ branching fraction, even at large $\tan\beta$. These features are illustrated in Figure~\ref{Figure:Chapter6_BranchingFractions}, which shows representative branching fraction maps for selected mass configurations. 

\begin{figure}[!htbp]
        \centering
        % First row
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/Phi_BR.pdf}
            \caption{}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/A_BR.pdf}
            \caption{}
        \end{subfigure}
    \caption[Branching fractions of $\phi$ and $A$ to $\PGt^+\PGt^-$ for selected mass combinations.]{Branching fractions of \textbf{(a)} $\phi \to \PGt^+\PGt^-$ and \textbf{(b)} $A \to \PGt^+\PGt^-$ as a function of $m_\phi$ and $m_A$, computed using \textsc{2HDECAY} in the alignment limit.}
    \label{Figure:Chapter6_BranchingFractions}
\end{figure}

\section{Event selection strategy}
\label{sec:ObjectAndEventSelections}

The analysis targets $Z^* \to \phi A \to 4\PGt$ decays, where the four taus can produce a wide range of final states depending on their decay modes. Figure~\ref{Figure:Chapter6_4tau_decayModes_BF} shows a pie chart of the most frequent four-tau final states, constructed by enumerating all combinations of leptonic and hadronic tau decays and summing their relative branching fractions. Channels containing two or more hadronic taus dominate the total branching fraction, contributing 87.1\% of all four-tau decays, and form the primary focus of the analysis. 

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{Figures/Chapter6/pie_BF.pdf}
    \caption[Branching fractions of the dominant four-$\PGt$ decay modes.]
    {Branching fractions of the most frequent $4\PGt$ final states, constructed by enumerating all combinations of leptonic ($\PGt_e$, $\PGt_{\mu}$) and hadronic ($\PGt_h$) tau decays.}
  \label{Figure:Chapter6_4tau_decayModes_BF}
\end{figure}

In contrast, Table~\ref{Table:Chapter6_4tau_decayModes_BF_Other} lists rarer final states with three or more leptonic taus, which are excluded from the signal regions due to their small branching fractions.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{12pt} % Increase column width
\arrayrulecolor{black} % Ensure outer border is black
\begin{tabular}{cc}
\hline
Decay Mode                  & Branching Fraction {[}\%{]} \\ \hline 
$\PGt_e \PGt_e \PGt_{\mu} \PGt_h$             & 4.3 \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_{\mu} \PGt_{\mu} \PGt_h$                    & 4.2 \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{e} \PGt_h$                        & 1.5  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_{\mu} \PGt_{\mu} \PGt_{\mu} \PGt_h$                  & 1.4  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{\mu} \PGt_{\mu}$             & 0.6  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{e} \PGt_{\mu}$                     & 0.4  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_{\mu} \PGt_{\mu} \PGt_{\mu}$             & 0.4  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_e \PGt_e \PGt_{e} \PGt_e$                  & 0.1  \\ 
\arrayrulecolor{lightgray} \hline
$\PGt_{\mu} \PGt_{\mu} \PGt_{\mu} \PGt_{\mu}$                  & 0.1  \\ 
\arrayrulecolor{black} \hline
\end{tabular}
\caption[Branching fractions of subdominant four-$\PGt$ decay modes]{Branching fractions of subdominant four-$\PGt$ final states, involving three or more leptonic $\PGt$ decays.}
\label{Table:Chapter6_4tau_decayModes_BF_Other}
\end{table}

To improve acceptance for events in which one $\PGt_h$ fails to be reconstructed, an orthogonal $\PGt_h\PGt_h\PGt_h$ channel is also included. This typically captures partially reconstructed $\PGt_h\PGt_h\PGt_h\PGt_h$ decays where one tau candidate fails trigger or identification requirements, often due to high $p_\text{T}$ thresholds or limited efficiency. 

In total, seven signal regions are defined based on final states with two or more hadronic taus. In addition, a control region is constructed using the fully leptonic $\PGt_{\mu} \PGt_{\mu} \PGt_{\mu} \PGt_{\mu}$ final state. This channel is highly pure and is used to constrain background modelling, as discussed in Section~\ref{Section:Chapter6_Background_Modelling}.

\subsection{Triggering}

Identifying events with four tau leptons presents a unique challenge, as no dedicated trigger exists for this topology. Instead, the selection strategy relies on combining standard single- and multi-object triggers that are sensitive to subsets of the final-state decay products. These include single-lepton (electron or muon), double-electron, double-muon, and double-tau triggers. Cross-triggers involving a lepton and a hadronic tau ($e+\tau_h$, $\mu+\tau_h$) and the electron–muon trigger were also evaluated, but offered negligible gains in signal acceptance and are therefore excluded to simplify the trigger efficiency evaluation.

Single-$\tau_h$ triggers, while available, are not used due to their high $p_\text{T}$ thresholds, which are too restrictive for the phase space relevant to this search. The triggers used in the analysis, along with the corresponding online $p_\text{T}$ thresholds for each data-taking year, are listed in Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt} % Increase column width
\begin{tabular}{|c|cc|cc|cc|}
\hline
\multirow{3}{*}{\text{Trigger}} 
& \multicolumn{6}{c|}{$p_\text{T}$ \text{Threshold (GeV)}} \\ \cline{2-7}
& \multicolumn{2}{c|}{\text{2016}} & \multicolumn{2}{c|}{\text{2017}} & \multicolumn{2}{c|}{\text{2018}} \\ \cline{2-7}
& \text{Obj$_1$} & \text{Obj$_2$} & \text{Obj$_1$} & \text{Obj$_2$} & \text{Obj$_1$} & \text{Obj$_2$} \\ \hline \hline
Single-Electron (e)                   & 26     & --     & 28     & --     & 33     & --     \\
\arrayrulecolor{lightgray} \hline
Single-Muon ($\mu$)                       & 23     & --     & 25     & --     & 25     & --     \\
\arrayrulecolor{lightgray} \hline
Double-Tau ($\PGt \PGt$)         & 40     & 40     & 40     & 40     & 40     & 40     \\
\arrayrulecolor{black} \hline
\end{tabular}
\caption{Table of minimum online $p_\text{T}$ thresholds (in GeV) for the triggers used in the analysis. Subcolumns refer to thresholds for the first and second trigger objects.}
\label{Table:Chapter6_TriggerThresholdsExpanded}
\end{table}

The trigger configuration applied to each channel is summarised in Table~\ref{Table:Chapter6_TriggersPerChannel}. For each channel, the trigger is considered to have fired if any listed object pair satisfies the corresponding online requirements. The logical OR operator ($\lor$) reflects the inclusive selection applied to possible object combinations.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\begin{tabular}{cc}
\hline
Channel & Trigger Configuration \\ \hline 

$\tau_h\tau_h\tau_h\tau_h$ &  
$\tau_1\tau_2 \mathbin{\lor} \tau_1\tau_3 \mathbin{\lor} \tau_1\tau_4 \mathbin{\lor} \tau_2\tau_3 \mathbin{\lor} \tau_2\tau_4 \mathbin{\lor} \tau_3\tau_4$ \\ 
\arrayrulecolor{lightgray} \hline

$\tau_h\tau_h\tau_h$ &
$\tau_1\tau_2 \mathbin{\lor} \tau_1\tau_3 \mathbin{\lor} \tau_2\tau_3$ \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_h\tau_h\tau_h$ &
$e_1 \mathbin{\lor} \tau_2\tau_3 \mathbin{\lor} \tau_2\tau_4 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_h\tau_h\tau_h$ &
$\mu_1 \mathbin{\lor} \tau_2\tau_3 \mathbin{\lor} \tau_2\tau_4 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_e\tau_h\tau_h$ &
$e_1 \mathbin{\lor} e_2 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_\mu\tau_h\tau_h$ &
$\mu_1 \mathbin{\lor} \mu_2 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_\mu\tau_h\tau_h$ &
$e_1 \mathbin{\lor} \mu_1 \mathbin{\lor} \tau_3\tau_4$ \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu \tau_\mu \tau_\mu \tau_\mu$ &
$\mu_1 \mathbin{\lor} \mu_2 \mathbin{\lor} \mu_3 \mathbin{\lor} \mu_4$ \\
\arrayrulecolor{black} \hline

\end{tabular}
\caption{Trigger configuration used for each four-$\PGt$ final state.}
\label{Table:Chapter6_TriggersPerChannel}
\end{table}

\subsection{Offline object selections}
\label{sec:ObjectSelection}

Offline selection criteria are applied to reconstructed electrons, muons, and hadronically decaying tau candidates, as introduced in Chapter~\ref{Section:Chapter4}. These selections suppress backgrounds from non-prompt and misidentified particles while maintaining high signal efficiency.

To ensure consistency with the PV and reduce contamination from PU, all selected objects are required to originate from the PV. This is enforced through cuts on transverse and longitudinal impact parameters: $|d_{xy}| < 0.045\unit{cm}$ and $|d_z| < 0.2\unit{cm}$ for electrons and muons, and $|d_z| < 0.2\unit{cm}$ for hadronic tau candidates.

Electrons and muons must be isolated from surrounding hadronic activity to suppress nonprompt backgrounds. The relative isolation variables used are defined in Chapter~\ref{Section:Chapter4}, Equations~\ref{Equation:Chapter4_PFIso_Electron} and~\ref{Equation:Chapter4_PFIso_Muon}, with a threshold of $I^{e/\mu}_\text{PF} < 0.15$. No explicit isolation cut is applied to hadronic tau candidates, as this information is embedded in the DeepTau discriminators.

Identification criteria follow those detailed in Chapter~\ref{Section:Chapter4}. Electrons are selected using a BDT-based discriminator targeting 90\% efficiency, and muons with the cut-based medium WP. For hadronic tau candidates, a looser identification is used to retain adequate signal yields. Candidates must pass the Loose working point of the $D_{\text{jet}}$ discriminator, along with lepton-rejection cuts: VVLoose for $D_e$ and VLoose for $D_\mu$. These thresholds are optimised to balance background rejection with signal retention and are summarised in Table~\ref{Table:Chapter4_DeepTau_WPs}.

To ensure consistency between trigger and offline reconstruction, selected objects must match the corresponding trigger-level objects within a cone of $\Delta R < 0.5$. The number of required matches depends on the trigger configuration for each channel, as outlined in Table~\ref{Table:Chapter6_TriggersPerChannel}. For channels using double-object triggers (\eg $\tauh\tauh\tauh\tauh$), any two $\tauh$ must be matched. In channels triggered by single leptons (\eg, $\tau_e\tauh\tauh\tauh$), only one matched object is required, depending on the path fired.

Stricter $p_T$ thresholds are applied to trigger-matched objects to ensure operation in the plateau of the trigger efficiency turn-on curve: $+1\GeV$ for electrons and muons, and $+5\GeV$ for hadronic tau candidates. Unmatched objects are subject to looser baseline thresholds. This conditional strategy maximises acceptance while preserving accurate modelling of trigger efficiencies.

A full summary of the offline selection criteria is given in Table~\ref{Table:Chapter6_ObjectSelectionSummary}.

{
\setlength{\arrayrulewidth}{1pt}

% Move the caption BEFORE the table
\begin{table}[!htbp]
\centering
\caption[Summary of baseline object selection criteria]{
Summary of baseline selection criteria applied to reconstructed electrons, muons, and hadronically decaying taus. Trigger-matched $p_T$ thresholds are defined relative to those in Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}.
}
\label{Table:Chapter6_ObjectSelectionSummary}

\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\arrayrulecolor{black}

\begin{tabular}{cccc}
\hline
Criteria & Electron & Muon & Hadronic Tau \\
\hline

$p_\text{T}$  & > $10\GeV$ & > $10\GeV$ & > $20\GeV$\\ 
\arrayrulecolor{lightgray} \hline

$p_\text{T}^{\text{Trigger}}$ & \multicolumn{3}{c}{$> \text{Table~\ref{Table:Chapter6_TriggerThresholdsExpanded}} + [1,\,1,\,5]$} \\
\arrayrulecolor{lightgray} \hline

$|\eta|$ & < $2.5$ & < $2.4$ & < $2.3$/$2.1$\hyperlink{DoubleTauTrigger-EtaCut}{$^1$} \\
\arrayrulecolor{lightgray} \hline

$|d_{xy}|$ & < $0.045\unit{cm}$ & < $0.045\unit{cm}$ & -- \\
\arrayrulecolor{lightgray} \hline

$|d_z|$ & < $0.2\unit{cm}$ & < $0.2\unit{cm}$ & < $0.2\unit{cm}$ \\
\arrayrulecolor{lightgray} \hline

Isolation & $I^e_\text{PF}$ < 0.15 & $I^\mu_\text{PF}$ < 0.15 & -- \\
\arrayrulecolor{lightgray} \hline

Identification
& \makecell{MVA w/o isolation\\(90\% WP)}
& Medium ID
& \makecell{
\normalfont\footnotesize$D_{\text{jet}} \geq$ Loose\hyperlink{Alternative-FFcut}{$^2$} \\
\normalfont\footnotesize$D_{e} \geq$ VVLoose \\
\normalfont\footnotesize$D_{\mu} \geq$ VLoose
} \\
\arrayrulecolor{black} \hline
\end{tabular}
\vspace{0.5em}
\begin{minipage}{0.95\linewidth}
\raggedright
\footnotesize
\hypertarget{DoubleTauTrigger-EtaCut}{}$^{1}$\,$|\eta| < 2.1$ is required for $\tauh$ candidates matched to trigger objects, reflecting the online trigger acceptance region. \\

\vspace{0.5em}

\hypertarget{Alternative-FFcut}{}$^{2}$\,The Loose WP is referred to as the \texttt{Nominal} tau identification in the fake factor method (see Section~\ref{Section:Chapter6_JetToTauBackground}). An \texttt{Alternative} selection, defined as $D_{\text{jet}} > 0.1$, is used to construct control regions.

\end{minipage}

\end{table}
}

\subsection{Event-level selections}

Beyond object-level requirements, additional criteria are imposed at the event level to improve signal purity and maintain statistical orthogonality between final states. These include constraints on the number and charge of reconstructed objects, as well as vetoes on additional leptons and $b$-tagged jets. The full set of event-level selection criteria for each final state is summarised in Table~\ref{Table:Chapter6_Event_Channel_Selections}.
{
\setlength{\arrayrulewidth}{1pt}

% Move the caption BEFORE the table
\begin{table}[!htbp]
\caption[Event-level selection requirements applied to each four-tau final state.]{
Event-level selection criteria applied to each four-tau final state. The table specifies the required number of electrons, muons, hadronic taus, and $b$-tagged jets, as well as the total charge requirement for the selected objects.}
\label{Table:Chapter6_Event_Channel_Selections}
\centering
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{12pt}
\arrayrulecolor{black}

\begin{tabular}{cccccc}
\hline
Channel & $\mathcal{N}_\text{electrons}$ & $\mathcal{N}_\text{muons}$ & $\mathcal{N}_{\PGt_h}$ & $\mathcal{N}_{b_\text{jets}}$\hyperlink{b-jet_selections}{$^1$} & $\sum\text{q}$\\
\hline

$\tau_h\tau_h\tau_h\tau_h$ &  0 & 0 & $\geq 4$ & $\geq 0$ & 0\\
\arrayrulecolor{lightgray} \hline

$\tau_h\tau_h\tau_h$ & 0 & 0 & 3 & $\geq 0$ & $\pm 1$\\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_h\tau_h\tau_h$ & 1 & 0 & $\geq3$ & $0$  & 0 \\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_h\tau_h\tau_h$ & 0 & 1 & $\geq 3$ & $0$ & 0 \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_e\tau_h\tau_h$ & 2 & 0 & $\geq 2$ & $0$ & 0\\
\arrayrulecolor{lightgray} \hline

$\tau_\mu\tau_\mu\tau_h\tau_h$ & 0 & 2 & $\geq 2$ & $0$ & 0 \\
\arrayrulecolor{lightgray} \hline

$\tau_e\tau_\mu\tau_h\tau_h$ & 1 & 1 & $\geq 2$ & $0$ & 0 \\
\arrayrulecolor{black} \hline
\end{tabular}
\vspace{0.5em}
\begin{minipage}{0.95\linewidth}
\raggedright
\footnotesize\hypertarget{b-jet_selections}{}$^{1}$\,Jets are required to pass the $b$-tagging criteria defined in Chapter~\ref{Section:Chapter4}, have $p_T > 30\GeV$, $|\eta| < 4.7$ and not overlap with any selected electron, muon or $\tauh$ candidates within $\Delta R < 0.5$.
\end{minipage}
\end{table}
}

These requirements are motivated by the assumptions of the signal model and optimised to suppress background contributions. In particular, $b$-jet vetoes are applied in channels containing light leptons to reduce $\ttbar$ contamination. Charge constraints are imposed to reflect the electrically neutral nature of the parent bosons ($Z^* \to \phi A$) and the expected tau multiplicity in the final state. The $\pm1$ charge requirement in the $\tauh\tauh\tauh$ channel accounts for one potentially unreconstructed tau.

\section{Object and event corrections}

Simulated events are subject to various imperfections, leading to discrepancies with data that originate from several sources. These include the limited precision of MC event generators, approximations in detector simulation, and differences in reconstruction performance. In particular, simulated objects may differ from those in data in their likelihood of passing selection criteria such as identification, isolation, or trigger requirements. Mismatches in detector response can also cause systematic shifts in reconstructed object energies.

To address these effects, a series of corrections is applied to simulated events to improve agreement with data. These include object-level adjustments, such as energy scale and resolution corrections, as well as event-level weights based on object-specific efficiency scale factors. For example, identification, isolation, and trigger efficiencies are measured separately for each reconstructed object and combined multiplicatively to yield a total event weight. 

Wherever possible, corrections provided centrally by the CMS collaboration are used. However, for analysis-specific selections or nonstandard trigger paths, dedicated corrections are derived to ensure consistency with the data-taking conditions and selection strategy employed. The remainder of this section summarises the key corrections applied to simulated events in this analysis.

\subsection{Pileup reweighting}

Accurate modelling of PU interactions is essential to reflect the conditions observed during data taking. In simulation, the PU profile, defined as the distribution of the mean number of interactions per bunch crossing, is constructed based on the expected instantaneous luminosity for each year. 

For each simulated event, a value of the mean PU, denoted by $\mu^{\text{PU}}$, is randomly drawn from this distribution. This value is treated as the mean of a Poisson distribution, from which the actual number of PU interactions is sampled. However, the simulated PU profile may differ from the true distribution observed in data. To account for this, a per-event weight is applied to reweight the simulation so that its PU distribution matches that of the data.

Figure~\ref{Figure:Chapter6_PU_Profiles} shows a comparison of the PU distributions in data and simulation during the CMS Run 2 data-taking period, illustrating the need for reweighting.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Figures/Chapter6/PU_Profile.pdf}
\caption{Comparison of the distributions of the mean number of proton-proton interactions per bunch crossing between data and simulation in CMS Run 2.}
\label{Figure:Chapter6_PU_Profiles}
\end{figure}

\subsection{Electron and Muon efficiencies}

Although simulated leptons are reconstructed using algorithms designed to replicate those used in data, residual differences arise from detector effects, reconstruction inefficiencies, and selection criteria. These are corrected using efficiency \ac{SF}.

The relevant efficiencies include tracking, identification, isolation, and trigger efficiencies. Tracking corrections are derived centrally by the CMS collaboration and applied directly. However, the remaining efficiencies are measured within this analysis using a dedicated implementation of the \textit{tag-and-probe} method~\cite{CMS_Muon_System_Performance,CMS_Muon_System_Performance_2}. This is necessary because the object selection criteria listed in Table~\ref{Table:Chapter6_ObjectSelectionSummary}, particularly the impact parameter cuts ($d_{xy}$ and $d_z$), differ from those used in the official CMS efficiency measurements.

The tag-and-probe technique is applied to leptons from $Z \rightarrow \ell^+\ell^-$ decays, providing a clean and unbiased sample for assessing selection performance. Events are selected by requiring an opposite-charge, same-flavour lepton pair with invariant mass $m_{\ell\ell}$ between $65\GeV$ and $115\GeV$, enhancing the $Z$ boson purity. One lepton, the \textit{tag}, is required to pass tight identification, isolation, and single-lepton trigger criteria, ensuring it does not bias the measurement of the second lepton, the \textit{probe}. Both leptons are alternately assigned as tag and probe to increase statistical precision.

Efficiencies are measured in bins of probe $p_{\mathrm{T}}$ and $\eta$ to account for detector and kinematic variations. The probe requirements for each selection stage are:

\begin{enumerate}[label=(\roman*)]
    \item Identification: Only requires identification
    \item Isolation: Requires identification, but not trigger.
    \item Trigger: Requires both identification and isolation.
\end{enumerate}

In each ($p_{\mathrm{T}}$, $\eta$) bin, events are split into ``pass'' and ``fail'' categories depending on whether the probe satisfies the selection under study. A simultaneous fit to the dilepton invariant mass distribution in both categories is used to extract the number of $Z \rightarrow \ell^+\ell^-$ signal events, accounting for residual background contamination. The signal is modelled with a sum of Voigtian functions, using the $Z$ boson’s natural width for the Breit-Wigner component. Backgrounds are modelled using:

\begin{itemize}
    \item A decaying exponential for the isolation and trigger fits.
    \item An error function transitioning to a decaying exponential for the identification fits, where background contamination is larger.
\end{itemize}

The efficiency is then extracted as:

\begin{equation_pad}
    \epsilon = \frac{N_{\text{pass}}}{N_{\text{pass}} + N_{\text{fail}}}
\end{equation_pad}

where $N_{\text{pass}}$ and $N_{\text{fail}}$ are the fitted $Z \rightarrow \ell^+\ell^-$ yields in each category. Figure~\ref{Figure:Chapter6-TagAndProbeFits_Nominal} shows an example fit for muons in a representative $\eta$ bin.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{Figures/Chapter6/data_id_pt_20_to_25_eta_0.0_to_0.8_tpzee_nominal.pdf}
\caption{Example of the tag-and-probe fit used to extract electron identification efficiency in data. This plot corresponds to the $p_{\mathrm{T}}$ bin $20–25\GeV$ and the $|\eta|$ region 0.0–0.8.}
\label{Figure:Chapter6-TagAndProbeFits_Nominal}
\end{figure}

Efficiency \acp{SF} are computed as:

\begin{equation_pad}
    \text{SF}(p_\text{T},\eta) = \frac{\epsilon_\text{data}(p_\text{T},\eta)}{\epsilon_\text{MC}(p_\text{T},\eta)}
\end{equation_pad}

These SFs are applied per object, and the total event-level correction is the product of the SFs for all leptons in the event.

To estimate systematic uncertainties, variations in the tag-and-probe procedure are explored. These include modifying the tag lepton $p_{\text{T}}$ threshold (\eg from $25\GeV$ to $35\GeV$), using alternative signal models, or changing the background parameterisation. Figure~\ref{Figure:Chapter6-TagAndProbeFits_Alternative} shows representative comparisons between these alternatives and the nominal fit configuration. The resulting variations in efficiency are propagated as systematic uncertainties in the SFs.

\begin{figure}[!htbp]
        \centering
        % First row
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/data_id_pt_20_to_25_eta_0.0_to_0.8_tpzee_signal.pdf}
            \caption{}
        \end{subfigure}
        \vspace{0.5cm}
        \begin{subfigure}[b]{\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/data_id_pt_20_to_25_eta_0.0_to_0.8_tpzee_tightTag.pdf}
            \caption{}
        \end{subfigure}
    \caption[Impact of tag-and-probe configuration variations on efficiency extraction.]{Impact of tag-and-probe configuration variations on the measurement of electron identification efficiency in the $p_{\mathrm{T}}$ bin $20–25\GeV$ and $|\eta|$ region 0.0–0.8. Shown are invariant mass distributions and corresponding fits for: \textbf{(a)} an alternative signal model, and \textbf{(b)} a tighter tag lepton $p_\text{T}$ selection. These are compared to the nominal configuration shown in Fig.~\ref{Figure:Chapter6-TagAndProbeFits_Nominal}.}
    \label{Figure:Chapter6-TagAndProbeFits_Alternative}
\end{figure}

\subsection{Hadronic tau efficiencies}

\subsubsection{Identification and Energy Scale}

Efficiency \acp{SF} are applied to simulated hadronic taus to correct for differences in reconstruction and identification performance between data and simulation. These corrections are derived centrally by the CMS collaboration and are therefore only summarised here.

The SFs are measured using a \textit{tag-and-probe} technique in events enriched with $\PZ/\gamma^* \to \tau_\mu \tau_h$ decays. The main observable is the visible mass of the muon–$\tau_h$ system, $m_{\text{vis}}$, evaluated over the range $50 < m_{\text{vis}} < 150\GeV$ to capture both the resonance and sidebands to help constrain the backgrounds. A binned maximum-likelihood fit to the $m_{\text{vis}}$ distribution is used to extract the SF by comparing data and simulation.

To improve the robustness of the SFs, the fit is performed simultaneously with a control region enriched in $\PZ/\gamma^* \to \mu\mu$ events. The DY yield is treated with a common rate parameter across both regions, while the hadronic tau identification SF is treated as the parameter of interest. Systematic uncertainties, including muon efficiency, luminosity, background normalisation, and MC statistics, are included as nuisance parameters. The resulting SFs are binned in decay mode for $\pt^{\tauh} > 20\GeV$, accounting for variations across decay topologies and kinematic regions. 

In parallel, the \ac{TES} is extracted using external template fits. For each decay mode, a set of $m_{\text{vis}}$ templates is generated with varied TES values, and the best-fit value is determined via a likelihood scan. This TES is then fixed when extracting the identification SFs. The resulting corrections typically fall within $\pm 2\%$, reflecting the high accuracy of CMS hadronic tau energy reconstruction. Both of these corrections are applied to hadronic taus matched to genuine hadronic tau decays at the generator level.

\subsubsection{Trigger}

Trigger efficiency corrections for hadronic taus candidates are derived using $\PZ/\gamma^* \to \tau_\mu \tau_h$ events, taking advantage of the clean topology and well-identified muon leg. These corrections are measured centrally by the CMS collaboration and are therefore only summarised here.

Efficiencies are extracted via a \textit{tag-and-probe} method, where the muon serves as the tag and the hadronic tau as the probe. To avoid bias on the probe leg, dedicated \textit{monitoring triggers} are used. These apply tight requirements to the muon while replicating the trigger logic under study for the tau-leg, allowing for a direct and unbiased efficiency measurement.

Corrections are provided separately for lepton+tau and double-tau triggers. The resulting scale factors are applied to simulated events to account for residual mismodelling in trigger performance.

\subsubsection{Lepton Misidentification Rates and Energy Scale}
\label{Section:Chapter6_Lepton_MisID_SF}

As described in Section~\ref{Section:Chapter6_Backgrounds}, electrons and muons can be misidentified as hadronic taus by the DeepTau algorithm. Corrections for these misidentification rates are derived centrally by the CMS collaboration and are summarised briefly here.

A \textit{tag-and-probe} method is used in $\PZ/\gamma^* \to ee$ and $\PZ/\gamma^* \to \mu\mu$ events, where one lepton serves as the tag and the other must be reconstructed as a hadronic tau. Events are categorised based on whether the probe passes or fails the relevant DeepTau discriminant ($D_e$ or $D_\mu$). The misidentification probability is measured as a function of $\eta_{\tauh}$, and the ratio between data and simulation defines the sSF. These SFs are applied to simulated events where an $e \to \tauh$ or $\mu \to \tauh$ misidentification occurs.

In addition, separate energy scale corrections are applied to hadronic taus originating from misidentified leptons to account for differences in reconstructed energy response between data and simulation.

\subsection{B-tagging efficiency}

To account for differences in $b$-tagging performance between data and simulation, jet-level corrections are applied using SFs derived centrally by the CMS collaboration. These SFs adjust the tagging efficiency for jets originating from $b$-quarks, $c$-quarks, and light-flavour partons.

Instead of applying a global event weight, the tagging status of each jet is modified individually. A random number generator is used to probabilistically promote or demote jets, depending on whether the SF is greater or less than one. This procedure ensures that the corrected simulation mirrors data-like behaviour, allowing direct use of the modified jet collection in subsequent selections and event categorisation steps without introducing additional per-event weights.


% This method is beneficial for analyses where the b-tag multiplicity defines the event category, since it allows for natural migration between categories and avoids complications associated with weighted event yields. However, it is not recommended in analyses that rely on variables which could become ill-defined when upgrading untagged jets—such as those involving secondary vertex information—since upgraded jets lack associated vertex reconstruction. Care must also be taken when SFs cross unity due to systematic variations, as this may affect the stability of such variables.


\section{Background Modelling}
\label{Section:Chapter6_Background_Modelling}

As described in Section~\ref{Section:Chapter6_Backgrounds}, several processes can mimic the $Z^* \to \phi A \to 4\PGt$ signal signature, either through the presence of genuine $\PGt$ leptons, misidentified jets or light leptons reconstructed as hadronic taus, or a combination of both. This section outlines the strategies used to model these backgrounds and validate their contributions. For clarity, background processes are grouped into three main categories:


\begin{enumerate}[label=(\roman*)]

\item \textbf{Genuine-$\PGt$ backgrounds:} Events containing only genuine $\PGt$ leptons, primarily from $\PZ\PZ \to 4\PGt$, form an irreducible background and are estimated using simulation.

\item \textbf{Jet $\to \PGt_h$ misidentification:} Backgrounds with one or more jets misidentified as $\PGt_h$ candidates are modelled using a data-driven fake factor method.

\item \textbf{Lepton $\to \PGt_h$ misidentification:} Although subdominant, backgrounds from electrons or muons misidentified as hadronic taus can affect specific final states. These are estimated using simulation with dedicated corrections, as detailed in Section~\ref{Section:Chapter6_Lepton_MisID_SF}.

\end{enumerate}

\subsection{\texorpdfstring{Genuine-\boldmath{$\PGt$} Backgrounds from $\PZ\PZ$}{Genuine tau Backgrounds from ZZ}}
\label{Section:Chapter6_GenuineBackground}


These backgrounds arise predominantly from quark–antiquark annihilation, with a smaller contribution from gluon–gluon fusion. Owing to their fully leptonic final state and close kinematic resemblance to the signal, they are particularly challenging to suppress. As listed in Table~\ref{Table:Chapter6_SimulatedBackgrounds}, the quark-initiated process is simulated using \POWHEG at NLO in QCD, while the gluon-initiated component is generated at LO with \PYTHIA. To account for missing higher-order effects, both samples are corrected using multiplicative $\mathcal{K}$-factors~\cite{Kfactors_ZZ} applied as a function of the four-lepton invariant mass, $m_{4\ell}$:


\begin{itemize}
\item Quark-initiated $\PZ\PZ$:  A mass-dependent NLO-to-NNLO QCD $\mathcal{K}$-factor derived from\POWHEG~2.0\cite{Powheg_1,Powheg_2} is applied, with a typical value of around 1.2 across the relevant phase space.
\item Gluon-initiated $\PZ\PZ$: A larger $\mathcal{K}$-factor, typically 2.0, is applied to match NNLO predictions from \textsc{HNNLO v2}\cite{PhysRevLett.98.222002}, also as a function of $m_{4\ell}$.
\end{itemize}

Simulation is validated in a control region enriched in $\PZ\PZ \to \mu^+\mu^-\mu^+\mu^-$ decays, which provides a clean signature with minimal background contamination. The standard object selection criteria (Table~\ref{Table:Chapter6_ObjectSelectionSummary}) are applied, with one targeted modification: the muon isolation requirement is relaxed to $I^\mu_\text{PF} < 0.35$ to enhance statistical precision. Additionally, the total charge of the four-muon system is required to be zero. The trigger configuration used in this region is detailed in Table~\ref{Table:Chapter6_TriggersPerChannel}. Figure~\ref{Figure:Chapter6_ZZ_KfactorImpact} illustrates how applying $\mathcal{K}$-factors improves the agreement between simulation and data in the invariant mass distribution of the four-muon system.

\begin{figure}[!htbp]
        \centering
        % First row
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mmmm_wo_kfactors.pdf}
            \caption{}
        \end{subfigure}
        \vspace{0.5cm}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/mmmm_with_kfactors.pdf}
            \caption{}
        \end{subfigure}
    \caption[Impact of $\mathcal{K}$-factors on the $\PZ\PZ$ background prediction in the four-muon control region.]{Impact of $\mathcal{K}$-factors on the $\PZ\PZ$ background prediction in the four-muon control region. Shown are the invariant mass distributions of the $\PGm^+\PGm^-\PGm^+\PGm^-$ system before (\textbf{a}) and after (\textbf{b}) applying $\mathcal{K}$-factors.}
    \label{Figure:Chapter6_ZZ_KfactorImpact}
\end{figure}

While $\PZ\PZ$ constitutes an irreducible background by construction, it can still be reduced through targeted kinematic selections. For example, in the $\PGt_e\PGt_h\PGt_h\PGt_h$ channel, signal events tend to exhibit harder $p_{\mathrm{T}}$ spectra for subleading leptons. The object selection thresholds are designed to exploit this difference. As illustrated in Figure~\ref{Figure:Chapter6_ThirdLepPt}, applying a $p_{\mathrm{T}}$ cut at $20\GeV$ significantly reduces the $\PZ\PZ$ background while preserving the majority of the signal.

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Chapter6/ZZ_OfflineCutImpact.pdf}
    \caption[$p_{\mathrm{T}}$ spectrum of the third lepton in the $\PGt_e\PGt_h\PGt_h\PGt_h$ channel.]{Distribution of the transverse momentum of the third lepton in the $\PGt_e\PGt_h\PGt_h\PGt_h$ final state, comparing the expected $\PZ\PZ$ background to the signal prediction. The solid line at $20\GeV$ region indicates the $p_{\mathrm{T}}$ selection threshold applied.}
    \label{Figure:Chapter6_ThirdLepPt}
\end{figure}

\subsection{\texorpdfstring{Background from Jets Misidentified as $\PGt_h$ (Jet $\to \PGt_h$)}{Background from Jets Misidentified as hadronic taus}}

\label{Section:Chapter6_JetToTauBackground}

This section discusses how the background from jets misidentified as $\PGt_h$ candidates is modelled. These backgrounds arise when jets from processes such as the ones outlined in Section~\ref{Section:Chapter6_Backgrounds} are misidentified as hadronic tau decays. Such misidentifications are particularly problematic to model in simulation, as the jet-to-$\PGt_h$ misidentification rate is poorly described in MC simulations. Moreover, the small probability of a jet being misidentified as a hadronic tau makes it necessary to generate high-statistics MC samples, which come with a significant computational expense. These limitations and the difficulty in capturing the full range of misidentification scenarios in simulation motivate the use of a data-driven approach, such as the Fake Factor ($F_F$) method.

\subsubsection{Classical Fake Factor Method}
\label{Section:Chapter6_FakeFactors_Classical}

The classical $F_F$ approach begins by defining a dedicated \ac{DR}, enriched in jet$\to\PGt_h$ events. Any non-jet$\to\PGt_h$ contributions are estimated using simulation and subtracted to isolate the misidentified component, ensuring a pure region. In this region, the $F_F$ is computed as the ratio of events passing a \texttt{Nominal} tau ID requirement to those failing it but satisfying a looser \texttt{Alternative} ID:

\begin{equation}
F_F = \frac{\mathcal{N}(\texttt{Nominal})}{\mathcal{N}(\texttt{Alternative and not Nominal})}
\end{equation}

The choice of the alternative identification requirement introduces a trade-off. Looser identification thresholds reduce the statistical uncertainty by increasing the number of events in the denominator. However, this comes at the cost of increased systematic uncertainty, as the looser $\PGt_h$ candidates are less representative of those in the SR. 

The $F_F$ is computed differentially in key variables such as $p_\text{T}$, jet multiplicity ($\mathcal{N}_{\text{jets}}$), and other observables that parametrise the misidentification rate. Since the $F_F$ can also depend on the definition of the DR, additional corrections are often derived in sideband regions to mitigate residual dependencies and improve the robustness of the estimate.

After derivation, the $F_F$ is applied to the \ac{AR}, which is constructed to resemble the SR. Specifically, the AR requires the hadronic taus to pass the \texttt{Alternative} ID and fail the \texttt{Nominal} one. This enables a fully data-driven estimate of the jet-induced background in the SR.

Despite its simplicity and robustness, the classical method faces several limitations:

\begin{itemize}
\item \textit{Curse of dimensionality}: Including more variables in the parameterisation rapidly inflates the number of bins, leading to sparse statistics and unstable estimates.

\item \textit{Correlations}: Variables not explicitly included in the fit may be poorly modelled. Corrections in one observable can worsen agreement in another, highlighting the limitations of low-dimensional approaches that fail to capture correlations.

\item \textit{Scalability in multi-$\PGt_h$ final states}: In channels like $\PGt_h \PGt_h \PGt_h \PGt_h$, the method becomes increasingly cumbersome. A separate $F_F$ must be computed for each hadronic tau, applied iteratively starting from the leading one. This procedure becomes statistically challenging in low-yield regimes and complicates the estimate.
\end{itemize}

These challenges have been highlighted in previous CMS analyses~\cite{CMS:2022goy,Mb:2022rxu}, where a $F_F$ was typically applied only to the leading hadronic tau. Events where the leading object was a genuine $\tau$ were corrected using simulation. This simplification was acceptable in those studies, as events with one genuine and one misidentified hadronic tau were subdominant. However, the final states probed in this analysis contain multiple misidentified hadronic taus, invalidating such assumptions. A more general, high-dimensional method is therefore required to consistently account for all misidentified tau contributions.

\subsubsection{Machine Learning-based reweighting}
\label{Section:Chapter6_FakeFactors_BDT}

To overcome the limitations of binning-based methods, misidentification rates can be estimated using \ac{ML}-based density ratio estimation. The objective is to model the misidentification rate as a function of multiple observables, corresponding mathematically to estimating the ratio of probability densities $f_{\text{pass}}(x)/f_{\text{fail}}(x)$.

\begin{equation_pad}
\frac{f_{\text{pass}}(x)}{f_{\text{fail}}(x)} \sim \frac{p_{\text{pass}}(x)}{p_{\text{fail}}(x)}.
\end{equation_pad}

This ratio can be approximated using classifiers such as \acp{BDT}, where the class probabilities serve as a proxy for the density ratio. However, standard classifiers often underperform in regions where the density ratio is large (\ie where one class dominates). In such regions, the classification task becomes trivial, and the model focuses its learning on harder, more ambiguous regions where classes are balanced. These easy-to-classify regions receive little weight in the loss function (typically cross-entropy), leading to poor modelling precisely where accurate reweighting is most critical.

To mitigate this, a custom objective function is introduced that directly targets discrepancies between the pass and fail distributions. In tree-based models, this objective is optimised over sets of events grouped into leaves\footnote{Leaves are the terminal nodes of a decision tree; each leaf contains events satisfying the same set of decision rules.}. The objective prioritises leaves with the largest differences between the two samples, using the symmetrised $\chi^2$ metric:

\begin{equation_pad}
    \chi^2 = \sum_{\text{leaf}} \frac{(w_{\text{leaf}, \text{fail}} - w_{\text{leaf}, \text{pass}})^2}{w_{\text{leaf}, \text{fail}} + w_{\text{leaf}, \text{pass}}}
\end{equation_pad}

where $w_{\text{leaf}, \text{fail}}$ and $w_{\text{leaf}, \text{pass}}$ are the total event weights in each leaf from the fail and pass samples, respectively. Maximising this objective during training steers the model toward the regions most relevant for improving reweighting accuracy.

Training proceeds in a boosting-like fashion, where a series of shallow decision trees is built sequentially. Each tree addresses discrepancies not corrected by previous ones. At each iteration, the following steps are performed:

\begin{itemize}
    \item A shallow decision tree that maximises the symmetrised $\chi^2$ is built.
    \item For each leaf, the log-ratio of total weights is computed:
    \begin{equation_pad}
        r_\text{leaf} = \log \left( \frac{w_{\text{leaf}, \text{fail}}}{w_{\text{leaf}, \text{pass}}} \right)
    \end{equation_pad}
    \item The weights of fail-sample events in each leaf are updated: $w = w \times e^{r{_\text{leaf}}}$.
\end{itemize}

This procedure is iterated multiple times, allowing the model to refine the reweighting progressively by targeting residual discrepancies in each step.

\subsubsection{Fitting and Parametrisation}

Having introduced the ML-based reweighting approach, this section outlines its implementation in the context of this analysis. This includes the definition of fitting regions and the choice of input variables for training.

As described in Section~\ref{Section:Chapter6_FakeFactors_Classical}, the classical $F_F$ method relies on defining a set of control regions. These regions can be organised into an ABCD-style framework, as illustrated in Figure~\ref{Figure:Chapter6_ABCD}, with sidebands constructed from variables such as hadronic tau identification and total charge.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.6\textwidth]{Figures/Chapter6/ABCD.pdf}
\caption[ABCD-style region definition for fake factor estimation.]{Schematic illustration of an ABCD-style region definition used in the fake factor method.}
\label{Figure:Chapter6_ABCD}
\end{figure}

The channel-specific sideband selections used to define these regions are summarised below:

\begin{enumerate}[label=(\roman*)]
    \item For the channels: $\PGt_e\PGt_h\PGt_h\PGt_h$, $\PGt_e\PGt_e\PGt_h\PGt_h$, $\PGt_e\PGt_\mu\PGt_h\PGt_h$, $\PGt_\mu\PGt_h\PGt_h\PGt_h$, $\PGt_\mu\PGt_\mu\PGt_h\PGt_h$, and $\PGt_h\PGt_h\PGt_h\PGt_h$:
    \begin{itemize}
    \item Region A (SR-like): All alternative\footnote{``Alternative'' refers to all other hadronic taus in the event for which the $F_F$ is not currently being derived.} hadronic taus pass the \texttt{Nominal} tau ID; $\sum q_{\PGt_h} = 0$.

        \item Region B (Inverted Identification): At least one alternative hadronic tau fails the \texttt{Nominal} tau identification but passes the \texttt{Alternative} selection; $\sum q_{\PGt_h} = 0$.

        \item Region C (Inverted Charge): All alternative $\PGt_h$ hadronic taus pass the \texttt{Nominal} tau ID; charge sum requirement is $\sum q_{\PGt_h} \neq 0$.

        \item Region D (Inverted ID and Charge): At least one alternative hadronic tau fails the \texttt{Nominal} tau ID but passes the \texttt{Alternative} selection; $\sum q_{\PGt_h} \neq 0$.
    \end{itemize}

    \item For the $\PGt_h\PGt_h\PGt_h$ channel, the same ID criteria apply, but the charge requirement is modified:
    \begin{itemize}
        \item Region A/B: $|\sum q_{\PGt_h}| = 1$
        \item Region C/D: $|\sum q_{\PGt_h}| \neq 1$
    \end{itemize}
\end{enumerate}

While this framework is conceptually straightforward when applied to a single hadronic tau, it becomes increasingly complex in multi-$\PGt_h$ events. Each one must be treated separately, with its own ABCD categorisation defined in terms of the others. This results in multiple overlapping sets of control regions that must be constructed within the same event, significantly increasing the dimensionality of the problem and fragmenting the dataset into many statistically limited subsets.

The ML approach circumvents these issues entirely by eliminating the need for explicit ABCD definitions. Each hadronic tau is treated as an independent training instance, contributing a single row to the training dataset. Both candidate-level and event-level features, including those used in traditional ABCD axes, are passed directly to the model. This enables the algorithm to learn a flexible, high-dimensional reweighting function across the entire parameter space without fragmenting the data.

The features used for training are grouped as follows:

\begin{enumerate}[label=(\roman*)]

    \item \textbf{Properties of the target $\PGt_h$ candidate:}
    \begin{itemize}
        \item HPS decay mode
        \item $p_\text{T}^{\PGt_h}$
        \item $p_\text{T}^\text{jet} / p_\text{T}^{\PGt_h}$, where the jet is the one seeding the HPS reconstruction
        \item $\eta$
        \item Charge
        \item Whether the candidate passes the corresponding leg of the double-$\tau$ trigger
    \end{itemize}

    \item \textbf{Sideband-specific variables:}
    \begin{itemize}
        \item Total charge of all $\PGt_h$ candidates
        \item A boolean indicating whether the DeepTau discriminator of the alternative hadronic taus (sorted by $p_\text{T}$) passes the \texttt{Nominal} tau identification requirement
    \end{itemize}

    \item \textbf{Global event variables:}
    \begin{itemize}
        \item Data-taking period identifier
        \item $p_\text{T}$ rank of the target hadronic tau in the event
    \end{itemize}

\end{enumerate}

\subsubsection{\texorpdfstring{Non-jet$\to \PGt_h$ background removal}{Non-jet to hadronic tau background removal}}

As noted in Section~\ref{Section:Chapter6_FakeFactors_Classical}, backgrounds from non-jet$\to\PGt_h$ candidates are traditionally removed by subtracting simulated events from data at the histogram level. However, this approach is not compatible with the ML-based method, which operates on full unbinned datasets and does not support negative event weights. One possible workaround is template matching, where a simulated non-jet$\to\PGt_h$ candidate for every matching data event is removed to emulate histogram subtraction at the event level. Yet, this method is computationally infeasible given the high dimensionality of the input space and the size of the datasets.

Instead, a binary BDT is trained to distinguish jet$\to\PGt_h$ from non-jet$\to\PGt_h$ candidates, using the same input variables as the main reweighting model. This reduces the classification problem to a single discriminant: the BDT score, which quantifies how "non-jet$\to\PGt_h$-like" each candidate is. The trained BDT is applied to both simulation and data. A histogram of BDT scores is constructed from simulated non-jet$\to\PGt_h$ candidates and normalised to the expected cross section, yielding a bin-by-bin prediction of their contamination in data. The same BDT is then applied to data, and for each bin of the BDT score distribution, data events falling within that range are randomly sampled and removed. This is repeated until the number of removed events matches the predicted contamination in that bin.

The result is a purified dataset of jet$\to\PGt_h$-like candidates, closely mimicking the effect of histogram-level subtraction but in a form that remains compatible with the ML-based reweighting method. Figure~\ref{Figure:Chapter6_BDTPurificationExample} compares this strategy with the traditional subtraction method, demonstrating their agreement.

\begin{figure}[!htbp]
        \centering
        % First row
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/subtraction_plot_q_sum_ttt_fail.pdf}
            \caption{}
        \end{subfigure}
        \vspace{0.5cm}
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{Figures/Chapter6/subtraction_plot_q_sum_ttt_pass.pdf}
            \caption{}
        \end{subfigure}
    \caption[Comparison of subtraction techniques applied to the total charge distribution of $\PGt_h$ candidates.]{Comparison of subtraction techniques applied to the total charge distribution ($\sum q_{\PGt_h}$) of $\PGt_h$ candidates in the \textbf{(a)} fail and \textbf{(b)} pass regions.}

    \label{Figure:Chapter6_BDTPurificationExample}
\end{figure}

\subsubsection{\texorpdfstring{Application of the $F_F$}{Application of the Fake Factor}}

In events with multiple hadronic tau candidates, the jet$\to\PGt_h$ background must account for all possible misidentification combinations. Simply applying the $F_F$ to the leading $\PGt_h$ candidate alone neglects cases where a subleading candidate is misidentified. On the other hand, applying the Fake Factor to all candidates simultaneously leads to overcounting. Hence, additive and subtractive combinations of the $F_F$ need to be considered to properly capture the full background. The specific combinations applied in events with up to two and three $\PGt_h$ candidates are summarised in Tables~\ref{Table:Chapter_6_FFApplication_2Taus} and~\ref{Table:Chapter_6_FFApplication_3Taus}, respectively.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{12pt} % Increase column width
\arrayrulecolor{black} % Ensure outer border is black
\begin{tabular}{cccc}
\hline
FF & $\PGt_h$ \textcolor{red}{$\PGt_h$} & \textcolor{red}{$\PGt_h$} $\PGt_h$ & \textcolor{red}{$\PGt_h \PGt_h$} \\ \hline

$F_F^1$ & 0 & 1 & 1 \\
\arrayrulecolor{lightgray} \hline

$F_F^2$ & 1 & 0 & 1 \\
\arrayrulecolor{lightgray} \hline

$-F_F^1 \times F_F^2$ & 0 & 0 & 1 \\
\arrayrulecolor{black} \hline
Combined & 1 & 1 & 1 \\
\end{tabular}
\caption[Combinatoric application of Fake Factors for multiple $\PGt_h$ candidates.]{Combinatoric application of Fake Factors in events with multiple $\PGt_h$ candidates. $R_i$ denotes the application region where only the $i$-th candidate is treated as a jet$\to\PGt_h$ object; $R_{ij}$ indicates regions where both candidates $i$ and $j$ are treated as fakes, and so on.}
\label{Table:Chapter_6_FFApplication_2Taus}
\end{table}


\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.5} % Increase row height
\setlength{\tabcolsep}{12pt} % Increase column width
\arrayrulecolor{black} % Ensure outer border is black
\begin{tabular}{ccccccc}
\hline
FF & \textcolor{red}{$\PGt_h$} $\PGt_h$ $\PGt_h$ & \textcolor{red}{$\PGt_h \PGt_h$} $\PGt_h$ & \textcolor{red}{$\PGt_h \PGt_h \PGt_h$} & $\PGt_h$ \textcolor{red}{$\PGt_h \PGt_h$} & $\PGt_h$ \textcolor{red}{$\PGt_h$} $\PGt_h$ & \textcolor{red}{$\PGt_h$} $\PGt_h$ \textcolor{red}{$\PGt_h$} \\ \hline

$F_F^1$ & 1 & 1 & 1 & 0 & 0 & 1 \\
\arrayrulecolor{lightgray} \hline

$F_F^2$ & 0 & 1 & 1 & 1 & 1 & 0 \\
\arrayrulecolor{lightgray} \hline

$F_F^3$ & 0 & 0 & 1 & 1 & 0 & 1 \\
\arrayrulecolor{lightgray} \hline

$-F_F^1 \times F_F^2$ & 0 & 1 & 1 & 0 & 0 & 0 \\
\arrayrulecolor{lightgray} \hline

$-F_F^1 \times F_F^3$ & 0 & 0 & 1 & 0 & 0 & 1 \\
\arrayrulecolor{lightgray} \hline

$-F_F^2 \times F_F^3$ & 0 & 0 & 1 & 1 & 0 & 0 \\
\arrayrulecolor{lightgray} \hline

$F_F^1 \times F_F^2 \times F_F^3$ & 0 & 0 & 1 & 0 & 0 & 0 \\
\arrayrulecolor{black} \hline

Combined & 1 & 1 & 1 & 1 & 1 & 1 \\
\end{tabular}
\caption[Combinatoric application of Fake Factors for multiple $\PGt_h$ candidates.]{Combinatoric application of Fake Factors in events with multiple $\PGt_h$ candidates. $R_i$ denotes the application region where only the $i$-th candidate is treated as a jet$\to\PGt_h$ object; $R_{ij}$ indicates regions where both candidates $i$ and $j$ are treated as fakes, and so on.}
\label{Table:Chapter_6_FFApplication_3Taus}
\end{table}

\section{Search strategy and statistical procedure}







